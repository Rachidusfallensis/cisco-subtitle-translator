{"1\n00:00:00,020 --> 00:00:07,260\nDoes performing this task require intelligence?\n\n2\n00:00:09,720 --> 00:00:10,240\nHow about this?\n\n3\n00:00:11,320 --> 00:00:13,840\nDoes this require intelligence?\n\n4\n00:00:15,760 --> 00:00:16,219\nHow about this?\n\n5\n00:00:19,020 --> 00:00:25,860\nSome tasks are harder than others,\n\n6\n00:00:26,360 --> 00:00:28,260\nso we'd say they require more intelligence.\n\n7\n00:00:29,120 --> 00:00:29,980\nLet's take a look at some tasks that we can do.\n\n8\n00:00:30,000 --> 00:00:35,400\ntasks that have different levels of difficulty to better understand what is considered intelligence.\n\n9\n00:00:36,140 --> 00:00:41,760\nHere is an image in a photo app. How would you instruct a computer to convert the image to\n\n10\n00:00:42,000 --> 00:00:47,860\ngrayscale? You can apply a math formula that takes an average of the red, green, and blue to convert\n\n11\n00:00:47,940 --> 00:00:53,360\nit to grayscale. Since this task can be accomplished pretty consistently with some math, we wouldn't\n\n12\n00:00:53,520 --> 00:00:58,920\nconsider this to acquire much intelligence. How about if you crop the image? If you give the machine\n\n13\n00:00:58,940 --> 00:01:04,920\nthe exact size and shape that you want to crop, the machine can perform this task well by following\n\n14\n00:01:05,160 --> 00:01:09,880\nsome simple rules. So this wouldn't require much intelligence. But what if you ask the machine\n\n15\n00:01:10,480 --> 00:01:16,100\nto choose how to crop the image? This task is more nuanced because the computer needs to figure out\n\n16\n00:01:16,320 --> 00:01:21,060\nwhat it should keep in the image and what it should leave out. For instance, it probably should keep\n\n17\n00:01:21,200 --> 00:01:27,060\nthe person and other relevant objects inside the image after cropping. So this may require some more\n\n18\n00:01:27,080 --> 00:01:32,500\nintelligence to do the task well. What if you wanted the computer to identify some of the objects\n\n19\n00:01:32,720 --> 00:01:37,440\nin the image and tell you what they are? For instance, I'd like to identify what these round\n\n20\n00:01:37,760 --> 00:01:43,160\nobjects are. This task is harder for a computer to do well with a few simple instructions because\n\n21\n00:01:43,380 --> 00:01:48,720\nthere are many other objects that are round and white, and so doing a simple match based on color\n\n22\n00:01:48,960 --> 00:01:54,340\nor shape probably won't be enough. You could imagine all the rules you might need to give the machine\n\n23\n00:01:54,360 --> 00:01:58,360\nand also exceptions to those rules in order to do this task well.\n\n24\n00:01:58,740 --> 00:02:02,060\nSo it's fair to say that this task requires more intelligence.\n\n25\n00:02:02,520 --> 00:02:05,080\nWhat if you want to make an object in the image disappear?\n\n26\n00:02:05,520 --> 00:02:09,160\nThis eraser lets me hand draw a circle around some object,\n\n27\n00:02:09,940 --> 00:02:11,620\nand it cleanly removes that object.\n\n28\n00:02:12,140 --> 00:02:14,800\nBut instead of just leaving an empty hole where the object was,\n\n29\n00:02:15,200 --> 00:02:20,200\nit fills in the missing space quite well with what would reasonably be behind that object.\n\n30\n00:02:20,580 --> 00:02:25,900\nAnd by the way, I was able to make that round steamed bun disappear without a photo app by eating it.\n\n31\n00:02:26,240 --> 00:02:30,900\nIf you imagine giving a computer instructions for how to do this, it would require a lot of instructions,\n\n32\n00:02:31,600 --> 00:02:38,920\nand even then, you might not even be able to think of all the instructions and exceptions that are needed to perform this task well in most situations.\n\n33\n00:02:39,420 --> 00:02:42,540\nSo this task probably requires more intelligence too.\n\n34\n00:02:42,860 --> 00:02:48,400\nOne way to think of intelligence is the ability to perform tasks that humans do well, such as\n\n35\n00:02:48,640 --> 00:02:53,820\nunderstanding images, text, and language. And it helps to think of these as a spectrum of tasks\n\n36\n00:02:54,140 --> 00:03:00,000\nthat may be easier or harder for a machine or even a human to do. An easier task would require\n\n37\n00:03:00,380 --> 00:03:06,120\nfewer rules, whereas a more difficult task would require more rules. You can think of the difficulty\n\n38\n00:03:06,260 --> 00:03:11,679\nof a task in terms of how many rules and exceptions to those rules you'd have to give a machine in\n\n39\n00:03:11,700 --> 00:03:17,160\norder for it to do the task well. To perform well on a difficult task, a person or computer would\n\n40\n00:03:17,380 --> 00:03:22,220\nideally be able to handle ambiguity. This means they can handle situations that they haven't\n\n41\n00:03:22,420 --> 00:03:27,360\nencountered before and still achieve a reasonable outcome. In the early days of AI research,\n\n42\n00:03:27,920 --> 00:03:32,980\nwriting these rules allowed machines to perform some useful tasks with images, text, and numbers.\n\n43\n00:03:33,360 --> 00:03:38,180\nBut what happens when giving lots of rules and instructions just isn't enough? Coming up next,\n\n44\n00:03:38,380 --> 00:03:43,440\nThere's another approach in AI that doesn't rely on humans to come up with these rules and instructions.\n\n45\n00:03:44,040 --> 00:03:45,240\nIt's called machine learning.\n\n46\n00:03:45,700 --> 00:03:47,320\nLet's look at machine learning next.\n\n": "1\n00:00:00,020 --> 00:00:07,259\nL'ex\u00e9cution de cette t\u00e2che n\u00e9cessite-t-elle de l'intelligence ?\n\n2\n00:00:09,720 --> 00:00:10,240\nQue pensez-vous de ceci ?\n\n3\n00:00:11,320 --> 00:00:13,839\nCela n\u00e9cessite-t-il de l'intelligence ?\n\n4\n00:00:15,759 --> 00:00:16,219\nQue pensez-vous de ceci ?\n\n5\n00:00:19,019 --> 00:00:25,859\nCertaines t\u00e2ches sont plus difficiles que d'autres,\n\n6\n00:00:26,359 --> 00:00:28,260\nNous dirions donc qu'ils requi\u00e8rent plus d'intelligence.\n\n7\n00:00:29,120 --> 00:00:29,980\nExaminons quelques t\u00e2ches que nous pouvons effectuer.\n\n8\n00:00:30,000 --> 00:00:35,399\ndes t\u00e2ches pr\u00e9sentant diff\u00e9rents niveaux de difficult\u00e9 afin de mieux comprendre ce que l'on consid\u00e8re comme de l'intelligence.\n\n9\n00:00:36,140 --> 00:00:41,759\nVoici une image dans une application photo. Comment pourriez-vous demander \u00e0 un ordinateur de convertir l'image en\n\n10\n00:00:42,000 --> 00:00:47,859\nniveaux de gris ? Vous pouvez appliquer une formule math\u00e9matique qui prend une moyenne du rouge, du vert et du bleu pour convertir les niveaux de gris.\n\n11\n00:00:47,939 --> 00:00:53,359\nen niveaux de gris. \u00c9tant donn\u00e9 que cette t\u00e2che peut \u00eatre accomplie de mani\u00e8re assez coh\u00e9rente \u00e0 l'aide de quelques calculs, nous n'avons pas l'intention d'en faire autant.\n\n12\n00:00:53,520 --> 00:00:58,920\nconsid\u00e9rer que cela permet d'acqu\u00e9rir beaucoup d'intelligence. Et si vous recadriez l'image ? Si vous donnez \u00e0 la machine\n\n13\n00:00:58,939 --> 00:01:04,920\nla taille et la forme exactes que vous souhaitez r\u00e9colter, la machine peut effectuer cette t\u00e2che en suivant les instructions suivantes\n\n14\n00:01:05,159 --> 00:01:09,879\nquelques r\u00e8gles simples. Cela ne n\u00e9cessiterait donc pas beaucoup d'intelligence. Mais que se passe-t-il si vous demandez \u00e0 la machine\n\n15\n00:01:10,480 --> 00:01:16,099\npour choisir comment recadrer l'image ? Cette t\u00e2che est plus nuanc\u00e9e car l'ordinateur doit d\u00e9terminer\n\n16\n00:01:16,319 --> 00:01:21,060\nce qu'il doit conserver dans l'image et ce qu'il doit en exclure. Par exemple, il devrait probablement conserver\n\n17\n00:01:21,200 --> 00:01:27,060\nla personne et les autres objets pertinents \u00e0 l'int\u00e9rieur de l'image apr\u00e8s recadrage. Cela peut donc n\u00e9cessiter un peu plus de\n\n18\n00:01:27,079 --> 00:01:32,500\nl'intelligence pour bien accomplir la t\u00e2che. Que se passerait-il si vous vouliez que l'ordinateur identifie certains objets ?\n\n19\n00:01:32,719 --> 00:01:37,439\ndans l'image et vous dire ce que c'est ? Par exemple, j'aimerais identifier ce que sont ces ronds.\n\n20\n00:01:37,759 --> 00:01:43,159\nLes objets sont. Il est plus difficile pour un ordinateur d'accomplir cette t\u00e2che avec quelques instructions simples parce que\n\n21\n00:01:43,379 --> 00:01:48,719\nil existe de nombreux autres objets ronds et blancs, et il est donc possible d'effectuer une simple correspondance sur la base de la couleur\n\n22\n00:01:48,960 --> 00:01:54,340\nou la forme ne suffira probablement pas. Vous pouvez imaginer toutes les r\u00e8gles que vous devrez donner \u00e0 la machine.\n\n23\n00:01:54,359 --> 00:01:58,359\nainsi que les exceptions \u00e0 ces r\u00e8gles afin de mener \u00e0 bien cette t\u00e2che.\n\n24\n00:01:58,740 --> 00:02:02,060\nIl est donc juste de dire que cette t\u00e2che requiert plus d'intelligence.\n\n25\n00:02:02,519 --> 00:02:05,079\nQue faire si l'on veut faire dispara\u00eetre un objet de l'image ?\n\n26\n00:02:05,519 --> 00:02:09,159\nCette gomme me permet de dessiner \u00e0 la main un cercle autour d'un objet,\n\n27\n00:02:09,939 --> 00:02:11,620\net il supprime proprement cet objet.\n\n28\n00:02:12,139 --> 00:02:14,800\nMais au lieu de laisser un trou vide \u00e0 l'endroit o\u00f9 se trouvait l'objet,\n\n29\n00:02:15,199 --> 00:02:20,199\nil remplit assez bien l'espace manquant avec ce qui devrait raisonnablement se trouver derri\u00e8re cet objet.\n\n30\n00:02:20,579 --> 00:02:25,900\nD'ailleurs, j'ai pu faire dispara\u00eetre ce petit pain rond \u00e0 la vapeur sans photo en le mangeant.\n\n31\n00:02:26,240 --> 00:02:30,900\nSi vous imaginez que vous donnez \u00e0 un ordinateur des instructions sur la mani\u00e8re de proc\u00e9der, il faudrait beaucoup d'instructions,\n\n32\n00:02:31,599 --> 00:02:38,920\net m\u00eame dans ce cas, il se peut que vous ne soyez pas en mesure de penser \u00e0 toutes les instructions et exceptions n\u00e9cessaires pour effectuer cette t\u00e2che correctement dans la plupart des situations.\n\n33\n00:02:39,420 --> 00:02:42,539\nCette t\u00e2che requiert donc probablement plus d'intelligence.\n\n34\n00:02:42,860 --> 00:02:48,400\nOn peut consid\u00e9rer l'intelligence comme la capacit\u00e9 \u00e0 effectuer des t\u00e2ches que les humains accomplissent bien, telles que\n\n35\n00:02:48,639 --> 00:02:53,819\nla compr\u00e9hension des images, des textes et du langage. Il est utile de consid\u00e9rer ces t\u00e2ches comme un \u00e9ventail de t\u00e2ches\n\n36\n00:02:54,139 --> 00:03:00,000\nqui peut \u00eatre plus ou moins facile \u00e0 r\u00e9aliser pour une machine ou m\u00eame pour un \u00eatre humain. Une t\u00e2che plus facile n\u00e9cessiterait\n\n37\n00:03:00,379 --> 00:03:06,120\nmoins de r\u00e8gles, alors qu'une t\u00e2che plus difficile n\u00e9cessiterait plus de r\u00e8gles. On peut penser \u00e0 la difficult\u00e9\n\n38\n00:03:06,259 --> 00:03:11,679\nd'une t\u00e2che en termes de nombre de r\u00e8gles et d'exceptions \u00e0 ces r\u00e8gles qu'il faudrait donner \u00e0 une machine dans le cadre d'un projet.\n\n39\n00:03:11,699 --> 00:03:17,159\npour qu'il puisse effectuer la t\u00e2che correctement. Pour r\u00e9ussir une t\u00e2che difficile, une personne ou un ordinateur doit\n\n40\n00:03:17,379 --> 00:03:22,219\ndans l'id\u00e9al, \u00eatre capable de g\u00e9rer l'ambigu\u00eft\u00e9. Cela signifie qu'ils peuvent faire face \u00e0 des situations qu'ils n'ont pas encore rencontr\u00e9es.\n\n41\n00:03:22,420 --> 00:03:27,360\nIl n'est pas rare de rencontrer des probl\u00e8mes de ce type dans le pass\u00e9 et d'obtenir un r\u00e9sultat raisonnable. Dans les premiers temps de la recherche sur l'IA,\n\n42\n00:03:27,920 --> 00:03:32,979\nL'\u00e9criture de ces r\u00e8gles a permis aux machines d'effectuer certaines t\u00e2ches utiles avec des images, du texte et des nombres.\n\n43\n00:03:33,360 --> 00:03:38,180\nMais que se passe-t-il lorsque donner beaucoup de r\u00e8gles et d'instructions ne suffit pas ? Prochainement,\n\n44\n00:03:38,379 --> 00:03:43,439\nIl existe une autre approche de l'IA qui ne d\u00e9pend pas de l'homme pour \u00e9laborer ces r\u00e8gles et instructions.\n\n45\n00:03:44,039 --> 00:03:45,240\nC'est ce qu'on appelle l'apprentissage automatique.\n\n46\n00:03:45,699 --> 00:03:47,319\nExaminons maintenant l'apprentissage automatique.\n\n", "1\n00:00:05,200 --> 00:00:11,340\nIf you can't see me, and also can't hear me, but can text with me, would you consider me\n\n2\n00:00:11,560 --> 00:00:17,080\nintelligent? What if you communicated with a computer with text messages? Would you consider\n\n3\n00:00:17,380 --> 00:00:21,260\nit intelligent? Let's take a look at what this has to do with artificial intelligence,\n\n4\n00:00:21,920 --> 00:00:25,000\nand look at how AI researchers have defined AI in the past.\n\n5\n00:00:25,620 --> 00:00:32,800\nTo define artificial intelligence, it helps to look at what some of the early founders of computer science and AI had to say about it.\n\n6\n00:00:33,120 --> 00:00:41,080\nAlan Turing is considered one of the founders of theoretical computer science, and he provided the mathematical theory behind the modern-day computer.\n\n7\n00:00:41,480 --> 00:00:44,680\nIn 1950, Turing wrote a paper in which he asked,\n\n8\n00:00:45,020 --> 00:00:54,100\nHe proposed a way of answering that question called the imitation game, in which a human observer has conversations by text with a human,\n\n9\n00:00:54,460 --> 00:00:57,480\nand also has a similar conversation with a machine.\n\n10\n00:00:57,920 --> 00:01:01,780\nThe observer cannot see or hear the other human or the machine,\n\n11\n00:01:02,460 --> 00:01:07,220\nso the observer has to decide purely on the quality of the text-based conversation\n\n12\n00:01:07,540 --> 00:01:11,480\nif the person on the other side is a human or not.\n\n13\n00:01:11,740 --> 00:01:15,680\nIf the human cannot distinguish between the machine and the other human,\n\n14\n00:01:16,040 --> 00:01:17,580\nwho is also communicating by text,\n\n15\n00:01:18,080 --> 00:01:21,180\nthen that machine demonstrates the ability to think.\n\n16\n00:01:21,560 --> 00:01:27,900\nThis imitation game is now also called the Turing Test. As a historical note, there was actually a\n\n17\n00:01:28,100 --> 00:01:33,260\npretty cool movie about Alan Turing and his team's efforts during the Second World War to decrypt the\n\n18\n00:01:33,500 --> 00:01:39,580\nmilitary communications of Nazi Germany. The name of the movie is The Imitation Game. The exact term\n\n19\n00:01:39,800 --> 00:01:44,980\nartificial intelligence was first coined by another founder of computer science, John McCarthy.\n\n20\n00:01:45,240 --> 00:01:54,640\nIn 1952, McCarthy and other scientists met at Dartmouth College to propose a study of whether machines can be designed to simulate intelligence.\n\n21\n00:01:55,240 --> 00:02:08,600\nThey wrote that their research would be based on the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it.\n\n22\n00:02:08,800 --> 00:02:15,280\nMcCarthy formally defined artificial intelligence as the science and engineering of making intelligent\n\n23\n00:02:15,600 --> 00:02:21,100\nmachines. But how do we define intelligence? McCarthy defined intelligence as the computational\n\n24\n00:02:21,480 --> 00:02:27,880\npart of the ability to achieve goals in the world. Another way to think about this is the ability to\n\n25\n00:02:28,040 --> 00:02:33,919\nperform useful tasks. So from a historical perspective, artificial intelligence is a field\n\n26\n00:02:33,940 --> 00:02:40,660\nof research that began in the 1950s with the goal of getting machines to perform tasks that require\n\n27\n00:02:40,920 --> 00:02:46,960\nintelligence. Let's break this down a bit. Artificial and machine means that it's not performed\n\n28\n00:02:47,260 --> 00:02:53,740\nby biological intelligence, like a human, a dog, or a panda. It's task-oriented. The machine is\n\n29\n00:02:53,860 --> 00:03:00,079\ntaking some action, like outputting a number or some text, to achieve a useful outcome. The third\n\n30\n00:03:00,100 --> 00:03:06,440\npart is intelligence. And defining intelligence can get somewhat philosophical, but intelligence\n\n31\n00:03:06,880 --> 00:03:12,760\nincludes the ability to make decisions even in ambiguous circumstances, and to learn from\n\n32\n00:03:13,080 --> 00:03:18,900\nexperience. Intelligence is often defined in terms of what humans are able to do, although it doesn't\n\n33\n00:03:19,040 --> 00:03:25,180\nhave to be limited to human intelligence. Next, let's take a closer look at defining intelligence.\n\n": "1\n00:00:05,200 --> 00:00:11,339\nSi vous ne pouvez pas me voir, ni m'entendre, mais que vous pouvez m'envoyer des SMS, me consid\u00e9rez-vous comme \u00e9tant\n\n2\n00:00:11,560 --> 00:00:17,079\nintelligent ? Et si vous communiquiez avec un ordinateur par le biais de messages textuels ? Consid\u00e9reriez-vous\n\n3\n00:00:17,379 --> 00:00:21,260\nest-elle intelligente ? Voyons ce que cela a \u00e0 voir avec l'intelligence artificielle,\n\n4\n00:00:21,920 --> 00:00:25,000\net examiner la mani\u00e8re dont les chercheurs en IA ont d\u00e9fini l'IA dans le pass\u00e9.\n\n5\n00:00:25,620 --> 00:00:32,799\nPour d\u00e9finir l'intelligence artificielle, il est utile d'examiner ce que certains des premiers fondateurs de l'informatique et de l'IA avaient \u00e0 dire \u00e0 ce sujet.\n\n6\n00:00:33,119 --> 00:00:41,079\nAlan Turing est consid\u00e9r\u00e9 comme l'un des fondateurs de l'informatique th\u00e9orique, et il a fourni la th\u00e9orie math\u00e9matique \u00e0 l'origine de l'ordinateur moderne.\n\n7\n00:00:41,479 --> 00:00:44,679\nEn 1950, Turing a r\u00e9dig\u00e9 un article dans lequel il posait la question suivante,\n\n8\n00:00:45,020 --> 00:00:54,100\nIl a propos\u00e9 un moyen de r\u00e9pondre \u00e0 cette question, le jeu d'imitation, dans lequel un observateur humain a des conversations textuelles avec un autre humain,\n\n9\n00:00:54,460 --> 00:00:57,479\net a \u00e9galement une conversation similaire avec une machine.\n\n10\n00:00:57,920 --> 00:01:01,780\nL'observateur ne peut ni voir ni entendre l'autre personne ou la machine,\n\n11\n00:01:02,460 --> 00:01:07,219\nl'observateur doit donc d\u00e9cider uniquement en fonction de la qualit\u00e9 de la conversation textuelle\n\n12\n00:01:07,540 --> 00:01:11,480\nsi la personne de l'autre c\u00f4t\u00e9 est un \u00eatre humain ou non.\n\n13\n00:01:11,739 --> 00:01:15,680\nSi l'homme ne peut pas faire la distinction entre la machine et l'autre homme,\n\n14\n00:01:16,039 --> 00:01:17,579\nqui communique \u00e9galement par texto,\n\n15\n00:01:18,079 --> 00:01:21,180\nalors cette machine d\u00e9montre sa capacit\u00e9 \u00e0 penser.\n\n16\n00:01:21,560 --> 00:01:27,900\nCe jeu d'imitation est aujourd'hui \u00e9galement appel\u00e9 le test de Turing. Pour l'anecdote, il existait en fait un\n\n17\n00:01:28,099 --> 00:01:33,259\nfilm assez sympa sur les efforts d'Alan Turing et de son \u00e9quipe pendant la Seconde Guerre mondiale pour d\u00e9crypter la\n\n18\n00:01:33,500 --> 00:01:39,579\nde la communication militaire de l'Allemagne nazie. Le film s'intitule The Imitation Game. Le terme exact\n\n19\n00:01:39,799 --> 00:01:44,979\nL'intelligence artificielle a \u00e9t\u00e9 invent\u00e9e par un autre fondateur de l'informatique, John McCarthy.\n\n20\n00:01:45,240 --> 00:01:54,640\nEn 1952, McCarthy et d'autres scientifiques se sont r\u00e9unis au Dartmouth College pour proposer d'\u00e9tudier la possibilit\u00e9 de concevoir des machines capables de simuler l'intelligence.\n\n21\n00:01:55,240 --> 00:02:08,599\nIls ont \u00e9crit que leur recherche serait bas\u00e9e sur la conjecture que chaque aspect de l'apprentissage ou toute autre caract\u00e9ristique de l'intelligence peut en principe \u00eatre d\u00e9crit avec une telle pr\u00e9cision qu'une machine peut \u00eatre fabriqu\u00e9e pour le simuler.\n\n22\n00:02:08,800 --> 00:02:15,280\nMcCarthy a formellement d\u00e9fini l'intelligence artificielle comme la science et l'ing\u00e9nierie permettant de rendre intelligents des objets et des personnes.\n\n23\n00:02:15,599 --> 00:02:21,099\ndes machines. Mais comment d\u00e9finir l'intelligence ? McCarthy a d\u00e9fini l'intelligence comme la capacit\u00e9 de calcul d'une machine.\n\n24\n00:02:21,479 --> 00:02:27,879\nfait partie de la capacit\u00e9 \u00e0 atteindre des objectifs dans le monde. Une autre fa\u00e7on de voir les choses est de consid\u00e9rer la capacit\u00e9 \u00e0\n\n25\n00:02:28,039 --> 00:02:33,918\neffectuer des t\u00e2ches utiles. D'un point de vue historique, l'intelligence artificielle est donc un domaine\n\n26\n00:02:33,939 --> 00:02:40,659\nde la recherche qui a d\u00e9but\u00e9 dans les ann\u00e9es 1950 dans le but de permettre aux machines d'effectuer des t\u00e2ches qui requi\u00e8rent de l'intelligence artificielle.\n\n27\n00:02:40,920 --> 00:02:46,960\nl'intelligence. D\u00e9taillons un peu ce concept. Artificiel et machine signifient qu'il n'est pas r\u00e9alis\u00e9\n\n28\n00:02:47,259 --> 00:02:53,740\npar une intelligence biologique, comme celle d'un humain, d'un chien ou d'un panda. Elle est orient\u00e9e vers les t\u00e2ches. La machine est\n\n29\n00:02:53,860 --> 00:03:00,079\neffectuer une action, comme produire un nombre ou un texte, pour obtenir un r\u00e9sultat utile. Le troisi\u00e8me\n\n30\n00:03:00,099 --> 00:03:06,439\nest l'intelligence. La d\u00e9finition de l'intelligence peut devenir quelque peu philosophique, mais l'intelligence\n\n31\n00:03:06,879 --> 00:03:12,759\ncomprend la capacit\u00e9 \u00e0 prendre des d\u00e9cisions, m\u00eame dans des circonstances ambigu\u00ebs, et \u00e0 tirer des enseignements de l'exp\u00e9rience acquise.\n\n32\n00:03:13,080 --> 00:03:18,900\nl'exp\u00e9rience. L'intelligence est souvent d\u00e9finie en fonction de ce que les humains sont capables de faire, m\u00eame si cela ne signifie pas qu'ils ne sont pas capables de faire quelque chose.\n\n33\n00:03:19,039 --> 00:03:25,180\nse limiter \u00e0 l'intelligence humaine. Examinons ensuite de plus pr\u00e8s la d\u00e9finition de l'intelligence.\n\n", "1\n00:00:07,100 --> 00:00:12,520\nOh, hello there. I was just enhancing my productivity. Really? Wait, wait, wait, hold on.\n\n2\n00:00:13,800 --> 00:00:19,060\nLet me explain something called incubation theory. What if you're working on a problem or a task,\n\n3\n00:00:19,520 --> 00:00:23,320\nand it's really hard, and you're spending a lot of time on it? One thing you could do\n\n4\n00:00:23,700 --> 00:00:28,240\nis to just keep trying to figure out the answer. Don't take breaks, don't sleep, don't eat,\n\n5\n00:00:28,380 --> 00:00:33,780\nand just keep working at it until you get the solution. Another approach is to take a break,\n\n6\n00:00:34,280 --> 00:00:40,580\nrelax, do something else, take a nap, get some food, maybe give yourself a day to sleep on it,\n\n7\n00:00:40,900 --> 00:00:46,100\nthen come back to the problem. What you might find when you try this out is that taking that break\n\n8\n00:00:46,600 --> 00:00:53,239\nactually helps you get the insight or new perspective to help you with that task. So why\n\n9\n00:00:53,260 --> 00:00:58,500\nis that psychologists have looked into why this happens, and here are what their studies suggest.\n\n10\n00:00:59,140 --> 00:01:04,220\nOne idea is called unconscious thought. Even when you're not actively thinking about solving a\n\n11\n00:01:04,460 --> 00:01:09,100\nparticular task, your mind is still processing that information. Occasionally, your mind will\n\n12\n00:01:09,320 --> 00:01:15,340\njump back in and maybe daydream about that topic or task. This time spent daydreaming helps you\n\n13\n00:01:15,560 --> 00:01:21,280\nthink differently to help you with your task. Another concept is called remote association.\n\n14\n00:01:21,860 --> 00:01:27,880\nFor example, let's say you're very familiar with ice cream and you're also very familiar with cakes.\n\n15\n00:01:28,440 --> 00:01:34,440\nOne day, you realize that you can combine ice cream and cake to create ice cream cake.\n\n16\n00:01:34,940 --> 00:01:42,280\nSo when you give yourself that time to step away from a task or problem, it allows your brain to make connections between things that may help you.\n\n17\n00:01:42,640 --> 00:01:45,420\nAnother idea is rest and recovery.\n\n18\n00:01:45,880 --> 00:01:52,280\nWhen you give yourself a break, this allows your brain to recover so that it can perform better when you go back to the task.\n\n19\n00:01:52,720 --> 00:01:55,440\nAnd there's also something called selective forgetting.\n\n20\n00:01:55,960 --> 00:02:00,600\nSometimes, when you're trying to solve a problem, you may take a path that leads to a dead end.\n\n21\n00:02:00,940 --> 00:02:08,380\nSelective forgetting, the way a goldfish is able to forget, may free up your brain to choose a different path to perform the task.\n\n22\n00:02:08,700 --> 00:02:14,080\nSo all of these ideas are collectively part of something psychologists call incubation theory.\n\n23\n00:02:14,580 --> 00:02:21,420\nIncubation theory suggests that you step away from your task, let it sit or incubate, and\n\n24\n00:02:21,520 --> 00:02:23,880\nthen when you come back, it will hatch into a baby chick.\n\n25\n00:02:24,440 --> 00:02:28,420\nOr you'll find more and possibly better ways to perform a task.\n\n26\n00:02:28,820 --> 00:02:30,740\nSo the steps you might take are the following.\n\n27\n00:02:31,140 --> 00:02:32,900\nFirst, try to solve the problem.\n\n28\n00:02:33,380 --> 00:02:36,440\nThis helps you better understand the problem and possible solutions.\n\n29\n00:02:37,100 --> 00:02:37,980\nThat's called preparation.\n\n30\n00:02:38,580 --> 00:02:39,320\nThen take a break.\n\n31\n00:02:39,800 --> 00:02:40,840\nThat's called incubation.\n\n32\n00:02:41,420 --> 00:02:43,920\nTaking a break could be switching to a different task.\n\n33\n00:02:44,220 --> 00:02:48,760\nPreferably something that allows you to free up your mind enough to daydream and occasionally\n\n34\n00:02:49,080 --> 00:02:50,980\nstill think about the task you're trying to solve.\n\n35\n00:02:51,280 --> 00:02:55,460\nIt could also include exercise, which also helps your brain's performance in general.\n\n36\n00:02:55,820 --> 00:02:58,380\nIt could be sleeping, such as taking short naps.\n\n37\n00:02:58,780 --> 00:03:04,200\nYou might find that you get some ideas in your dreams or as you're waking up from a\n\n38\n00:03:04,340 --> 00:03:04,360\ndream.\n\n39\n00:03:04,800 --> 00:03:10,180\nAfter you give your brain some time to incubate, then you reach the idea that's called illumination.\n\n40\n00:03:10,680 --> 00:03:14,780\nThen you can check if your solution, your new idea, is correct and if it works.\n\n41\n00:03:15,260 --> 00:03:16,800\nThat's called the verification step.\n\n42\n00:03:17,520 --> 00:03:20,440\nSo next time you're feeling stuck, maybe take a short break.\n\n43\n00:03:21,300 --> 00:03:23,460\nAnd you may find that's exactly what you needed.\n\n44\n00:03:24,480 --> 00:03:27,580\nComing up next, you'll learn about machine translation.\n\n": "1\n00:00:07,099 --> 00:00:12,519\nOh, bonjour. J'\u00e9tais juste en train d'am\u00e9liorer ma productivit\u00e9. Vraiment ? Attendez, attendez, attendez, attendez.\n\n2\n00:00:13,800 --> 00:00:19,059\nPermettez-moi de vous expliquer ce qu'on appelle la th\u00e9orie de l'incubation. Vous travaillez sur un probl\u00e8me ou une t\u00e2che,\n\n3\n00:00:19,519 --> 00:00:23,320\net c'est vraiment difficile, et vous y consacrez beaucoup de temps ? Une chose que vous pourriez faire\n\n4\n00:00:23,699 --> 00:00:28,239\nest de continuer \u00e0 essayer de trouver la r\u00e9ponse. Ne faites pas de pause, ne dormez pas, ne mangez pas,\n\n5\n00:00:28,379 --> 00:00:33,780\net continuez \u00e0 travailler jusqu'\u00e0 ce que vous obteniez la solution. Une autre approche consiste \u00e0 faire une pause,\n\n6\n00:00:34,280 --> 00:00:40,579\nrelax, do something else, take a nap, get some food, maybe give yourself a day to sleep on it,\n\n7\n00:00:40,899 --> 00:00:46,100\nthen come back to the problem. What you might find when you try this out is that taking that break\n\n8\n00:00:46,600 --> 00:00:53,238\nvous aide en fait \u00e0 obtenir la vision ou la nouvelle perspective qui vous aidera \u00e0 accomplir cette t\u00e2che. Alors, pourquoi\n\n9\n00:00:53,259 --> 00:00:58,500\nest que des psychologues se sont pench\u00e9s sur les raisons de ce ph\u00e9nom\u00e8ne, et voici ce que leurs \u00e9tudes sugg\u00e8rent.\n\n10\n00:00:59,140 --> 00:01:04,219\nOne idea is called unconscious thought. Even when you're not actively thinking about solving a\n\n11\n00:01:04,459 --> 00:01:09,099\nSi vous \u00eates en train d'effectuer une t\u00e2che particuli\u00e8re, votre esprit est encore en train de traiter cette information. Occasionnellement, votre esprit\n\n12\n00:01:09,319 --> 00:01:15,340\njump back in and maybe daydream about that topic or task. This time spent daydreaming helps you\n\n13\n00:01:15,560 --> 00:01:21,280\nthink differently to help you with your task. Another concept is called remote association.\n\n14\n00:01:21,859 --> 00:01:27,879\nFor example, let's say you're very familiar with ice cream and you're also very familiar with cakes.\n\n15\n00:01:28,439 --> 00:01:34,439\nUn jour, vous vous rendez compte que vous pouvez combiner de la cr\u00e8me glac\u00e9e et du g\u00e2teau pour cr\u00e9er un g\u00e2teau \u00e0 la cr\u00e8me glac\u00e9e.\n\n16\n00:01:34,939 --> 00:01:42,280\nAinsi, lorsque vous vous donnez le temps de vous \u00e9loigner d'une t\u00e2che ou d'un probl\u00e8me, vous permettez \u00e0 votre cerveau d'\u00e9tablir des liens entre des \u00e9l\u00e9ments susceptibles de vous aider.\n\n17\n00:01:42,640 --> 00:01:45,420\nAnother idea is rest and recovery.\n\n18\n00:01:45,879 --> 00:01:52,280\nLorsque vous vous accordez une pause, vous permettez \u00e0 votre cerveau de r\u00e9cup\u00e9rer et d'\u00eatre plus performant lorsque vous reprenez la t\u00e2che.\n\n19\n00:01:52,719 --> 00:01:55,439\nAnd there's also something called selective forgetting.\n\n20\n00:01:55,960 --> 00:02:00,599\nSometimes, when you're trying to solve a problem, you may take a path that leads to a dead end.\n\n21\n00:02:00,939 --> 00:02:08,379\nL'oubli s\u00e9lectif, \u00e0 la mani\u00e8re d'un poisson rouge, peut permettre \u00e0 votre cerveau de choisir un autre chemin pour accomplir la t\u00e2che.\n\n22\n00:02:08,699 --> 00:02:14,080\nToutes ces id\u00e9es font partie de ce que les psychologues appellent la th\u00e9orie de l'incubation.\n\n23\n00:02:14,580 --> 00:02:21,420\nIncubation theory suggests that you step away from your task, let it sit or incubate, and\n\n24\n00:02:21,520 --> 00:02:23,879\nthen when you come back, it will hatch into a baby chick.\n\n25\n00:02:24,439 --> 00:02:28,420\nOu vous trouverez d'autres moyens, voire de meilleurs moyens, d'effectuer une t\u00e2che.\n\n26\n00:02:28,819 --> 00:02:30,740\nSo the steps you might take are the following.\n\n27\n00:02:31,139 --> 00:02:32,900\nFirst, try to solve the problem.\n\n28\n00:02:33,379 --> 00:02:36,439\nThis helps you better understand the problem and possible solutions.\n\n29\n00:02:37,099 --> 00:02:37,979\nThat's called preparation.\n\n30\n00:02:38,579 --> 00:02:39,319\nThen take a break.\n\n31\n00:02:39,800 --> 00:02:40,840\nC'est ce qu'on appelle l'incubation.\n\n32\n00:02:41,420 --> 00:02:43,920\nTaking a break could be switching to a different task.\n\n33\n00:02:44,219 --> 00:02:48,759\nPreferably something that allows you to free up your mind enough to daydream and occasionally\n\n34\n00:02:49,079 --> 00:02:50,979\nstill think about the task you're trying to solve.\n\n35\n00:02:51,280 --> 00:02:55,460\nIt could also include exercise, which also helps your brain's performance in general.\n\n36\n00:02:55,819 --> 00:02:58,379\nIt could be sleeping, such as taking short naps.\n\n37\n00:02:58,780 --> 00:03:04,199\nYou might find that you get some ideas in your dreams or as you're waking up from a\n\n38\n00:03:04,340 --> 00:03:04,360\ndream.\n\n39\n00:03:04,800 --> 00:03:10,180\nApr\u00e8s avoir laiss\u00e9 votre cerveau incuber, vous parvenez \u00e0 l'id\u00e9e que l'on appelle l'illumination.\n\n40\n00:03:10,680 --> 00:03:14,780\nThen you can check if your solution, your new idea, is correct and if it works.\n\n41\n00:03:15,259 --> 00:03:16,800\nThat's called the verification step.\n\n42\n00:03:17,520 --> 00:03:20,439\nSo next time you're feeling stuck, maybe take a short break.\n\n43\n00:03:21,300 --> 00:03:23,460\nVous d\u00e9couvrirez peut-\u00eatre que c'est exactement ce dont vous aviez besoin.\n\n44\n00:03:24,479 --> 00:03:27,579\nProchainement, vous apprendrez ce qu'est la traduction automatique.\n\n", "1\n00:00:05,080 --> 00:00:09,220\nHave you ever picked up a receipt or a brochure or seen a sign on the wall and found yourself\n\n2\n00:00:09,520 --> 00:00:14,940\ntyping the text into your phone? What if instead you could take a picture of it and then let your\n\n3\n00:00:15,120 --> 00:00:20,420\nphone get the text from it for you? Let's take a look at optical character recognition and how it\n\n4\n00:00:20,480 --> 00:00:25,640\nlets you do that. Let's say you took a picture of a sign that you thought was interesting and you\n\n5\n00:00:25,660 --> 00:00:30,600\nwant to send it to your friend, Frances Ko. Frances replies that she can't read what the\n\n6\n00:00:30,680 --> 00:00:35,660\nsign says and asks you to type it out. So you could try to type it out by hand, but then you remember\n\n7\n00:00:35,940 --> 00:00:40,800\nthat you can use computer vision on your phone. So if you're using Google Photos, you can click on\n\n8\n00:00:40,900 --> 00:00:44,920\nthe lens icon, then it looks for parts of the image that look like characters of a language,\n\n9\n00:00:45,420 --> 00:00:49,160\nin this case English. Then it lets you select the text that you want to further process.\n\n10\n00:00:49,880 --> 00:00:54,620\nSo then you can click copy. Then you can go back to your message to Frances, paste in the text,\n\n11\n00:00:54,700 --> 00:00:59,120\nand send it. So this kind of computer vision task that looks for characters of a language\n\n12\n00:00:59,700 --> 00:01:04,620\nand attempts to identify and recognize those characters in order to output regular text,\n\n13\n00:01:05,100 --> 00:01:12,060\nthis is called optical character recognition, often shortened to the acronym OCR. So what is OCR\n\n14\n00:01:12,280 --> 00:01:17,100\ndoing? The software identifies a bounding box around each part of the image that appears to\n\n15\n00:01:17,340 --> 00:01:23,319\ncontain a character in a language. Then, for each bounding box, it classifies that as one of those\n\n16\n00:01:23,340 --> 00:01:28,600\nletters. So optical character recognition is actually an application of object detection,\n\n17\n00:01:29,100 --> 00:01:34,260\nbecause it's first localizing where the text appears in an image, and then it's identifying\n\n18\n00:01:34,640 --> 00:01:40,560\nwhether it's letter A, B, C, D, all the way to Z, as well as punctuation like commas, periods,\n\n19\n00:01:41,000 --> 00:01:44,900\ndollar signs, and so on. So I'd like to go back and point out one thing you might have noticed.\n\n20\n00:01:45,300 --> 00:01:49,999\nThe text that you got from optical character recognition looked like this, and you can see here\n\n21\n00:01:50,060 --> 00:01:56,020\nthe text doesn't seem quite understandable. First, there's this line, Leawood Walk of F,\n\n22\n00:01:56,240 --> 00:02:01,780\nand below that, just Holly. Let's go back to the image and take a closer look. If you zoom in,\n\n23\n00:02:02,080 --> 00:02:06,960\nhere's part of the image that the software is having trouble with. The text is written on the\n\n24\n00:02:07,120 --> 00:02:12,880\ninside of a circle. Here you can see that Leawood Walk of Fame, and here is Holly. So why do you\n\n25\n00:02:12,940 --> 00:02:18,379\nthink it output the text like this? Well, most text, like in a document or a book, is written in\n\n26\n00:02:18,400 --> 00:02:23,480\nstraight lines. And for many languages like English, text flows from left to right, then top\n\n27\n00:02:23,600 --> 00:02:29,180\nto bottom. So it looks like the OCR software was detecting text from top to bottom, and it assumed\n\n28\n00:02:29,500 --> 00:02:35,860\nthat Leawood walk of F was on one line, and the next line below that was Holly. And so one common\n\n29\n00:02:36,100 --> 00:02:40,760\ntheme that you'll see throughout this course is that even though AI-enabled software can do pretty\n\n30\n00:02:40,920 --> 00:02:46,739\ncool things, you as the user and the human still play an important role in checking whether the\n\n31\n00:02:46,760 --> 00:02:51,440\noutput makes sense, so it's still up to you to make adjustments as necessary. The other point I'd\n\n32\n00:02:51,440 --> 00:02:55,400\nlike to make is that when something doesn't work, it might be a chance to learn. For instance,\n\n33\n00:02:55,920 --> 00:03:00,240\nwhen something like this isn't working quite right, if you can figure out why it's not working the\n\n34\n00:03:00,380 --> 00:03:04,760\nway you expect it to, then you might get a better understanding of how this application or this\n\n35\n00:03:04,840 --> 00:03:09,280\nsoftware works and how you can use it. Another scenario where you could use optical character\n\n36\n00:03:09,480 --> 00:03:14,159\nrecognition is when you take pictures of your receipts after you go to a restaurant or grocery\n\n37\n00:03:14,180 --> 00:03:19,480\nstore. You can use OCR to extract the items that you purchased, as well as the prices that you paid\n\n38\n00:03:19,600 --> 00:03:24,080\nfor them. And what if you're traveling and you took a picture of this sign at an outdoor food market?\n\n39\n00:03:24,580 --> 00:03:29,900\nThis app has another button, Translate, and it looks like it performs OCR on parts of the image\n\n40\n00:03:30,240 --> 00:03:35,040\nwith Chinese characters, then translate that text into English and overlays the English on top of\n\n41\n00:03:35,140 --> 00:03:39,779\nthat. You'll learn more about machine translation later on in the rest of this course. You can also\n\n42\n00:03:39,800 --> 00:03:44,920\napply these computer vision tasks that you saw in the previous couple videos on your computer inside\n\n43\n00:03:45,120 --> 00:03:50,080\na web browser. If you use Google Chrome web browser, you can open up the menu on the top right and then\n\n44\n00:03:50,280 --> 00:03:56,160\nclick on \"Search with Google Lens\". Select the area you want to search or extract text from and you\n\n45\n00:03:56,240 --> 00:04:01,440\ncan copy that text. You can also translate that text and select objects and images to search as\n\n46\n00:04:01,440 --> 00:04:06,659\nwell. Coming up next, I'll share some tips on why taking a break may help you with your studies and\n\n47\n00:04:06,680 --> 00:04:11,980\nyour work. So please take a short break and then check out the next video to see why it's good for you.\n\n": "1\n00:00:05,080 --> 00:00:09,220\nVous est-il d\u00e9j\u00e0 arriv\u00e9 de prendre un ticket de caisse ou une brochure ou de voir une affiche sur un mur et de vous retrouver\n\n2\n00:00:09,519 --> 00:00:14,939\nen tapant le texte sur votre t\u00e9l\u00e9phone ? Et si, au lieu de cela, vous pouviez le prendre en photo et laisser votre t\u00e9l\u00e9phone portable s'en charger ?\n\n3\n00:00:15,119 --> 00:00:20,420\nphone en tire le texte qu'il vous faut ? Jetons un coup d'\u0153il \u00e0 la reconnaissance optique de caract\u00e8res et \u00e0 la mani\u00e8re dont elle est utilis\u00e9e.\n\n4\n00:00:20,480 --> 00:00:25,640\nvous permet de le faire. Supposons que vous ayez pris une photo d'un panneau qui vous a paru int\u00e9ressant et que vous ayez\n\n5\n00:00:25,660 --> 00:00:30,600\nwant to send it to your friend, Frances Ko. Frances replies that she can't read what the\n\n6\n00:00:30,679 --> 00:00:35,659\net vous demande de le taper. Vous pouvez donc essayer de le taper \u00e0 la main, mais vous vous souviendrez alors que vous n'avez pas le temps.\n\n7\n00:00:35,939 --> 00:00:40,799\nque vous pouvez utiliser la vision artificielle sur votre t\u00e9l\u00e9phone. Si vous utilisez Google Photos, vous pouvez cliquer sur\n\n8\n00:00:40,899 --> 00:00:44,920\nl'ic\u00f4ne de l'objectif, il recherche les parties de l'image qui ressemblent aux caract\u00e8res d'une langue,\n\n9\n00:00:45,420 --> 00:00:49,159\nen l'occurrence l'anglais. Il vous permet ensuite de s\u00e9lectionner le texte que vous souhaitez traiter.\n\n10\n00:00:49,880 --> 00:00:54,619\nVous pouvez alors cliquer sur copier. Vous pouvez ensuite retourner dans votre message \u00e0 Frances et coller le texte,\n\n11\n00:00:54,700 --> 00:00:59,119\nand send it. So this kind of computer vision task that looks for characters of a language\n\n12\n00:00:59,700 --> 00:01:04,620\net tente d'identifier et de reconna\u00eetre ces caract\u00e8res afin de produire un texte r\u00e9gulier,\n\n13\n00:01:05,099 --> 00:01:12,060\nthis is called optical character recognition, often shortened to the acronym OCR. So what is OCR\n\n14\n00:01:12,280 --> 00:01:17,099\nfaire ? Le logiciel identifie un cadre autour de chaque partie de l'image qui semble\n\n15\n00:01:17,340 --> 00:01:23,319\ncontain a character in a language. Then, for each bounding box, it classifies that as one of those\n\n16\n00:01:23,340 --> 00:01:28,599\nlettres. La reconnaissance optique de caract\u00e8res est donc en fait une application de la d\u00e9tection d'objets,\n\n17\n00:01:29,099 --> 00:01:34,259\nbecause it's first localizing where the text appears in an image, and then it's identifying\n\n18\n00:01:34,640 --> 00:01:40,560\nqu'il s'agisse de la lettre A, B, C, D, jusqu'\u00e0 Z, ainsi que la ponctuation comme les virgules, les points,\n\n19\n00:01:41,000 --> 00:01:44,900\nles signes du dollar, etc. J'aimerais donc revenir en arri\u00e8re et souligner une chose que vous avez peut-\u00eatre remarqu\u00e9e.\n\n20\n00:01:45,299 --> 00:01:49,998\nThe text that you got from optical character recognition looked like this, and you can see here\n\n21\n00:01:50,060 --> 00:01:56,020\nthe text doesn't seem quite understandable. First, there's this line, Leawood Walk of F,\n\n22\n00:01:56,240 --> 00:02:01,780\nand below that, just Holly. Let's go back to the image and take a closer look. If you zoom in,\n\n23\n00:02:02,079 --> 00:02:06,959\nVoici une partie de l'image qui pose probl\u00e8me au logiciel. Le texte est \u00e9crit sur le\n\n24\n00:02:07,120 --> 00:02:12,879\ninside of a circle. Here you can see that Leawood Walk of Fame, and here is Holly. So why do you\n\n25\n00:02:12,939 --> 00:02:18,378\nthink it output the text like this? Well, most text, like in a document or a book, is written in\n\n26\n00:02:18,400 --> 00:02:23,479\nstraight lines. And for many languages like English, text flows from left to right, then top\n\n27\n00:02:23,599 --> 00:02:29,180\nde haut en bas. Il semble donc que le logiciel d'OCR ait d\u00e9tect\u00e9 le texte de haut en bas, et qu'il ait suppos\u00e9 que l'OCR n'\u00e9tait pas en mesure de d\u00e9tecter le texte de haut en bas.\n\n28\n00:02:29,500 --> 00:02:35,860\nthat Leawood walk of F was on one line, and the next line below that was Holly. And so one common\n\n29\n00:02:36,099 --> 00:02:40,759\ntheme that you'll see throughout this course is that even though AI-enabled software can do pretty\n\n30\n00:02:40,920 --> 00:02:46,739\nde l'utilisateur et de l'\u00eatre humain joue un r\u00f4le important dans la v\u00e9rification de l'efficacit\u00e9 de l'utilisation de l'appareil.\n\n31\n00:02:46,759 --> 00:02:51,439\noutput makes sense, so it's still up to you to make adjustments as necessary. The other point I'd\n\n32\n00:02:51,439 --> 00:02:55,400\nest que lorsque quelque chose ne fonctionne pas, c'est peut-\u00eatre l'occasion d'apprendre. Par exemple,\n\n33\n00:02:55,920 --> 00:03:00,240\nwhen something like this isn't working quite right, if you can figure out why it's not working the\n\n34\n00:03:00,379 --> 00:03:04,759\nway you expect it to, then you might get a better understanding of how this application or this\n\n35\n00:03:04,840 --> 00:03:09,280\nsoftware works and how you can use it. Another scenario where you could use optical character\n\n36\n00:03:09,479 --> 00:03:14,158\nrecognition is when you take pictures of your receipts after you go to a restaurant or grocery\n\n37\n00:03:14,180 --> 00:03:19,479\nstore. You can use OCR to extract the items that you purchased, as well as the prices that you paid\n\n38\n00:03:19,599 --> 00:03:24,079\nfor them. And what if you're traveling and you took a picture of this sign at an outdoor food market?\n\n39\n00:03:24,579 --> 00:03:29,900\nThis app has another button, Translate, and it looks like it performs OCR on parts of the image\n\n40\n00:03:30,240 --> 00:03:35,039\nwith Chinese characters, then translate that text into English and overlays the English on top of\n\n41\n00:03:35,139 --> 00:03:39,778\nthat. You'll learn more about machine translation later on in the rest of this course. You can also\n\n42\n00:03:39,800 --> 00:03:44,920\nappliquer les t\u00e2ches de vision par ordinateur que vous avez vues dans les deux vid\u00e9os pr\u00e9c\u00e9dentes sur votre ordinateur \u00e0 l'int\u00e9rieur\n\n43\n00:03:45,120 --> 00:03:50,079\na web browser. If you use Google Chrome web browser, you can open up the menu on the top right and then\n\n44\n00:03:50,280 --> 00:03:56,159\nclick on \"Search with Google Lens\". Select the area you want to search or extract text from and you\n\n45\n00:03:56,240 --> 00:04:01,439\npeut copier ce texte. Vous pouvez \u00e9galement traduire ce texte et s\u00e9lectionner des objets et des images \u00e0 rechercher en tant que\n\n46\n00:04:01,439 --> 00:04:06,658\nwell. Coming up next, I'll share some tips on why taking a break may help you with your studies and\n\n47\n00:04:06,680 --> 00:04:11,979\nyour work. So please take a short break and then check out the next video to see why it's good for you.\n\n", "1\n00:00:05,080 --> 00:00:09,080\nIf you want to know what kind of flower this is, one thing you can do is to take a picture of it\n\n2\n00:00:09,320 --> 00:00:14,540\nand then get your photo app to tell you what it is. In this case, it says it's a peony. So how does\n\n3\n00:00:14,680 --> 00:00:19,360\nthe app do that? Let's take a look at computer vision, and in particular, object detection,\n\n4\n00:00:19,800 --> 00:00:25,519\nto find out more. So here's a photo that I'm viewing inside the Google Photos app. On the\n\n5\n00:00:25,540 --> 00:00:31,000\nbottom here, you can click on Lens, and you'll see that it creates a rectangle around an object\n\n6\n00:00:31,420 --> 00:00:37,140\nthat it detected. And in this case, it's me. One common task in computer vision is to identify\n\n7\n00:00:37,680 --> 00:00:43,260\na subsection of an image that contains an object of interest. It does this by defining a rectangle\n\n8\n00:00:43,640 --> 00:00:49,120\ncalled a bounding box that surrounds that object. And theoretically, it could also create a bounding\n\n9\n00:00:49,260 --> 00:00:54,740\nbox around other objects of interest as well. For example, maybe over here, or maybe down here,\n\n10\n00:00:54,900 --> 00:00:59,540\nor maybe even back up here. This task of defining bounding boxes around objects of interest\n\n11\n00:00:59,980 --> 00:01:05,780\nis called object localization. Something I'd like to mention is that these bounding boxes are\n\n12\n00:01:06,100 --> 00:01:12,180\nusually rectangles and not other shapes like circles, triangles, or even parallelograms,\n\n13\n00:01:12,460 --> 00:01:17,680\nbecause rectangles are fairly efficient to define and work with. Another thing I'd like to note is\n\n14\n00:01:17,780 --> 00:01:23,679\nthat object localization just identifies a region with something of interest, but doesn't yet include\n\n15\n00:01:23,700 --> 00:01:28,960\nthe step of identifying what is within that bounding box. In other words, it tells you where\n\n16\n00:01:29,340 --> 00:01:35,000\nan object is within an image, but not what that object is. Here's another photograph. If I click\n\n17\n00:01:35,200 --> 00:01:40,680\non lens here and then scroll down a bit, you can see that it classified this image to be either a\n\n18\n00:01:40,900 --> 00:01:48,579\nMaltese dog or a terrier. And by the way, his name is Tofu. This is another common task in computer\n\n19\n00:01:48,600 --> 00:01:50,560\nvision called image classification.\n\n20\n00:01:51,400 --> 00:01:56,460\nOne thing to note here is that the task of classification does not include the creation\n\n21\n00:01:56,700 --> 00:01:57,380\nof the bounding box.\n\n22\n00:01:57,960 --> 00:02:01,740\nEven though in this demonstration the app has also created a bounding box around the\n\n23\n00:02:01,880 --> 00:02:07,400\ndog, the task of classification is just focused on identifying the name or label of an image.\n\n24\n00:02:07,740 --> 00:02:11,420\nThe possible set of outputs of an image classifier are called classes.\n\n25\n00:02:12,080 --> 00:02:18,160\nSome examples of classes could be person, flower, dog, or bird, for instance.\n\n26\n00:02:18,620 --> 00:02:23,920\nTo identify where an object is and what it is, you can combine these two tasks.\n\n27\n00:02:24,720 --> 00:02:27,340\nObject localization gets us these bounding boxes.\n\n28\n00:02:27,800 --> 00:02:29,440\nThat's where these objects are located.\n\n29\n00:02:29,920 --> 00:02:34,460\nThen you can apply image classification on the subset of the image that's within each\n\n30\n00:02:34,640 --> 00:02:35,080\nbounding box.\n\n31\n00:02:35,740 --> 00:02:39,980\nSo you get person, glasses, flower, tree, for instance.\n\n32\n00:02:40,620 --> 00:02:46,160\nThis combination of object localization and image classification is called object detection.\n\n33\n00:02:46,620 --> 00:02:51,140\nNotice that for a single photograph, object detection can detect multiple objects.\n\n34\n00:02:51,760 --> 00:02:54,900\nThere are some things worth noting when you're using an object detection app.\n\n35\n00:02:55,260 --> 00:02:57,380\nYou may get different specificity in the class.\n\n36\n00:02:58,220 --> 00:03:03,400\nFor instance, the class will probably be more specific than just \"flower\", but here you\n\n37\n00:03:03,480 --> 00:03:06,740\ncan see that this yellow flower is identified as a dahlia.\n\n38\n00:03:07,200 --> 00:03:13,200\nhere you can see that this class is slightly more specific. It's a Dahlia pinata. I did some\n\n39\n00:03:13,440 --> 00:03:18,580\nsearching online and I think this yellow Dahlia is called a golden torch Dahlia. Something else\n\n40\n00:03:18,740 --> 00:03:23,660\nthat I think is helpful to know is that the camera angle or point of view can affect the results.\n\n41\n00:03:24,060 --> 00:03:29,519\nFor instance, this picture of this cute puppy is facing the front of his face. The image classifier\n\n42\n00:03:29,620 --> 00:03:38,540\nsuggests that it may be a Feist, Podcake Dog, Cur, Hound, Haspen, or even a Pitbull. If I use a\n\n43\n00:03:38,820 --> 00:03:43,580\npicture of the same dog at a different angle, in this case this shows its body a bit more,\n\n44\n00:03:43,920 --> 00:03:49,020\nthen the image classifier suggests that it might be a Mountain View Cur, or a Cajun Squirrel Dog,\n\n45\n00:03:49,440 --> 00:03:55,639\nor Black Mouth Cur. And by the way, this puppy's name is Neo. You may also see different or better\n\n46\n00:03:55,660 --> 00:04:01,860\nresults if you adjust the bounding box manually. For instance, here, given the default bounding box,\n\n47\n00:04:02,200 --> 00:04:06,760\nyou don't see any text that suggests what type of bird it is, you just see some similar looking\n\n48\n00:04:06,940 --> 00:04:11,220\nimages, and you'll learn more about these search results in the next video. But if you manually\n\n49\n00:04:11,540 --> 00:04:15,700\nadjust the bounding box, usually making it fit more closely around the object, then you might\n\n50\n00:04:15,900 --> 00:04:21,200\nget better results. So now, the image classifier gives some suggested classes: Humboldt Penguin,\n\n51\n00:04:21,920 --> 00:04:27,960\nAfrican penguin, and penguin. So you've probably noticed that below the image classifier results,\n\n52\n00:04:28,520 --> 00:04:33,260\nyou can see similar images. The photo app is performing another type of task\n\n53\n00:04:33,740 --> 00:04:38,720\ncalled similarity search. Let's take a look at similarity search in the next video.\n\n": "1\n00:00:05,080 --> 00:00:09,080\nIf you want to know what kind of flower this is, one thing you can do is to take a picture of it\n\n2\n00:00:09,320 --> 00:00:14,539\nand then get your photo app to tell you what it is. In this case, it says it's a peony. So how does\n\n3\n00:00:14,679 --> 00:00:19,359\nl'application fait-elle cela ? Jetons un coup d'\u0153il \u00e0 la vision par ordinateur, et en particulier \u00e0 la d\u00e9tection d'objets,\n\n4\n00:00:19,800 --> 00:00:25,518\npour en savoir plus. Voici donc une photo que je visualise dans l'application Google Photos. Sur la page\n\n5\n00:00:25,539 --> 00:00:31,000\nEn bas, vous pouvez cliquer sur Lens, et vous verrez que cela cr\u00e9e un rectangle autour d'un objet\n\n6\n00:00:31,420 --> 00:00:37,140\nqu'il a d\u00e9tect\u00e9. Et dans ce cas, c'est moi. Une t\u00e2che courante dans le domaine de la vision par ordinateur consiste \u00e0 identifier\n\n7\n00:00:37,679 --> 00:00:43,259\na subsection of an image that contains an object of interest. It does this by defining a rectangle\n\n8\n00:00:43,640 --> 00:00:49,119\nappel\u00e9e \"bo\u00eete de d\u00e9limitation\" qui entoure cet objet. Et th\u00e9oriquement, il pourrait \u00e9galement cr\u00e9er une bo\u00eete de d\u00e9limitation qui entoure l'objet.\n\n9\n00:00:49,259 --> 00:00:54,740\nbox around other objects of interest as well. For example, maybe over here, or maybe down here,\n\n10\n00:00:54,899 --> 00:00:59,539\nor maybe even back up here. This task of defining bounding boxes around objects of interest\n\n11\n00:00:59,979 --> 00:01:05,780\ns'appelle la localisation d'objets. J'aimerais mentionner que ces bo\u00eetes de d\u00e9limitation sont\n\n12\n00:01:06,099 --> 00:01:12,180\ng\u00e9n\u00e9ralement des rectangles et non d'autres formes comme des cercles, des triangles ou m\u00eame des parall\u00e9logrammes,\n\n13\n00:01:12,460 --> 00:01:17,680\nbecause rectangles are fairly efficient to define and work with. Another thing I'd like to note is\n\n14\n00:01:17,780 --> 00:01:23,679\nque la localisation d'objets ne fait qu'identifier une r\u00e9gion o\u00f9 se trouve un objet int\u00e9ressant, mais ne comprend pas encore les \u00e9l\u00e9ments suivants\n\n15\n00:01:23,700 --> 00:01:28,960\nthe step of identifying what is within that bounding box. In other words, it tells you where\n\n16\n00:01:29,340 --> 00:01:35,000\nun objet se trouve dans une image, mais pas ce qu'est cet objet. Voici une autre photographie. Si je clique sur\n\n17\n00:01:35,200 --> 00:01:40,680\nsur l'objectif ici et faire d\u00e9filer un peu vers le bas, vous pouvez voir qu'il a class\u00e9 cette image comme \u00e9tant soit un\n\n18\n00:01:40,900 --> 00:01:48,579\nMaltese dog or a terrier. And by the way, his name is Tofu. This is another common task in computer\n\n19\n00:01:48,599 --> 00:01:50,560\nvision appel\u00e9e classification d'images.\n\n20\n00:01:51,400 --> 00:01:56,460\nOne thing to note here is that the task of classification does not include the creation\n\n21\n00:01:56,700 --> 00:01:57,379\nde la bo\u00eete de d\u00e9limitation.\n\n22\n00:01:57,960 --> 00:02:01,739\nM\u00eame si, dans cette d\u00e9monstration, l'application a \u00e9galement cr\u00e9\u00e9 une bo\u00eete englobante autour de l'objet\n\n23\n00:02:01,879 --> 00:02:07,400\nchien, la t\u00e2che de classification est simplement ax\u00e9e sur l'identification du nom ou de l'\u00e9tiquette d'une image.\n\n24\n00:02:07,739 --> 00:02:11,419\nL'ensemble des sorties possibles d'un classificateur d'images est appel\u00e9 classe.\n\n25\n00:02:12,080 --> 00:02:18,159\nDes exemples de classes peuvent \u00eatre la personne, la fleur, le chien ou l'oiseau, par exemple.\n\n26\n00:02:18,620 --> 00:02:23,920\nTo identify where an object is and what it is, you can combine these two tasks.\n\n27\n00:02:24,719 --> 00:02:27,340\nLa localisation des objets nous permet d'obtenir ces bo\u00eetes de d\u00e9limitation.\n\n28\n00:02:27,800 --> 00:02:29,439\nThat's where these objects are located.\n\n29\n00:02:29,920 --> 00:02:34,460\nThen you can apply image classification on the subset of the image that's within each\n\n30\n00:02:34,639 --> 00:02:35,079\nbo\u00eete de d\u00e9limitation.\n\n31\n00:02:35,740 --> 00:02:39,979\nSo you get person, glasses, flower, tree, for instance.\n\n32\n00:02:40,620 --> 00:02:46,159\nThis combination of object localization and image classification is called object detection.\n\n33\n00:02:46,620 --> 00:02:51,139\nNotice that for a single photograph, object detection can detect multiple objects.\n\n34\n00:02:51,759 --> 00:02:54,900\nThere are some things worth noting when you're using an object detection app.\n\n35\n00:02:55,259 --> 00:02:57,379\nYou may get different specificity in the class.\n\n36\n00:02:58,219 --> 00:03:03,400\nPar exemple, la classe sera probablement plus sp\u00e9cifique qu'une simple \"fleur\", mais ici vous\n\n37\n00:03:03,479 --> 00:03:06,740\ncan see that this yellow flower is identified as a dahlia.\n\n38\n00:03:07,199 --> 00:03:13,199\nhere you can see that this class is slightly more specific. It's a Dahlia pinata. I did some\n\n39\n00:03:13,439 --> 00:03:18,579\nJ'ai fait des recherches en ligne et je crois que ce dahlia jaune est appel\u00e9 dahlia torche d'or. Autre chose\n\n40\n00:03:18,740 --> 00:03:23,659\nJe pense qu'il est utile de savoir que l'angle de la cam\u00e9ra ou le point de vue peut affecter les r\u00e9sultats.\n\n41\n00:03:24,060 --> 00:03:29,519\nFor instance, this picture of this cute puppy is facing the front of his face. The image classifier\n\n42\n00:03:29,620 --> 00:03:38,539\nsugg\u00e8re qu'il peut s'agir d'un Feist, d'un Podcake Dog, d'un Cur, d'un Hound, d'un Haspen ou m\u00eame d'un Pitbull. Si j'utilise un\n\n43\n00:03:38,819 --> 00:03:43,579\npicture of the same dog at a different angle, in this case this shows its body a bit more,\n\n44\n00:03:43,920 --> 00:03:49,020\nle classificateur d'images sugg\u00e8re qu'il pourrait s'agir d'un Mountain View Cur ou d'un Cajun Squirrel Dog,\n\n45\n00:03:49,439 --> 00:03:55,639\nou Black Mouth Cur. Au fait, ce chiot s'appelle Neo. Vous pouvez aussi voir d'autres ou de meilleurs\n\n46\n00:03:55,659 --> 00:04:01,860\nsi vous ajustez manuellement la bo\u00eete de d\u00e9limitation. Par exemple, ici, compte tenu de la bo\u00eete de d\u00e9limitation par d\u00e9faut,\n\n47\n00:04:02,199 --> 00:04:06,759\nvous ne voyez pas de texte indiquant de quel type d'oiseau il s'agit, vous voyez simplement des oiseaux qui se ressemblent.\n\n48\n00:04:06,939 --> 00:04:11,219\nVous en saurez plus sur ces r\u00e9sultats de recherche dans la prochaine vid\u00e9o. Cependant, si vous effectuez des recherches manuelles sur des images, vous pourrez obtenir des r\u00e9sultats plus pr\u00e9cis.\n\n49\n00:04:11,539 --> 00:04:15,699\najuster la bo\u00eete de d\u00e9limitation, en g\u00e9n\u00e9ral pour qu'elle s'ajuste plus \u00e9troitement \u00e0 l'objet, puis vous pourriez\n\n50\n00:04:15,900 --> 00:04:21,199\nget better results. So now, the image classifier gives some suggested classes: Humboldt Penguin,\n\n51\n00:04:21,920 --> 00:04:27,959\nAfrican penguin, and penguin. So you've probably noticed that below the image classifier results,\n\n52\n00:04:28,519 --> 00:04:33,259\nvous pouvez voir des images similaires. L'application photo effectue un autre type de t\u00e2che\n\n53\n00:04:33,740 --> 00:04:38,720\ncalled similarity search. Let's take a look at similarity search in the next video.\n\n", "1\n00:00:05,140 --> 00:00:09,960\nWhen you're using an object detection service such as Google Lens, you'll notice that below\n\n2\n00:00:10,200 --> 00:00:14,700\nthe suggested classes of the image classifier, you'll see search results for similar images.\n\n3\n00:00:15,380 --> 00:00:21,040\nSo, these white fluffy dogs look similar to the original white fluffy dog. Similarly,\n\n4\n00:00:21,540 --> 00:00:26,960\nafter the suggested classes for this cute brown dog, you can see similar images of other cute\n\n5\n00:00:26,980 --> 00:00:33,320\nbrown dogs. This is called similarity search, and you can apply this to finding similar images and\n\n6\n00:00:33,500 --> 00:00:39,680\nalso to finding similar audio or similar text. Let's get a brief conceptual overview of how\n\n7\n00:00:40,020 --> 00:00:46,360\nsimilarity search works. What if you tried to use a single number to represent any image in order to\n\n8\n00:00:46,500 --> 00:00:56,940\nhelp you compare and contrast any two images? Let's just pick a feature and draw a number line. For\n\n9\n00:00:56,960 --> 00:01:03,800\nSo for this dog, you might assign it a number 1, whereas for this dog, you might assign it a number 6.\n\n10\n00:01:04,220 --> 00:01:11,060\nSo you give this dog a higher number because in the image, it appears to have more brown fur compared to the other dog.\n\n11\n00:01:11,580 --> 00:01:15,240\nNow what if you can represent the image with not one, but two numbers?\n\n12\n00:01:15,960 --> 00:01:23,300\nSo in addition to the amount of brown fur, you could draw another number line to represent the amount of white fur.\n\n13\n00:01:23,780 --> 00:01:29,360\nSo this image of the first dog here might have a number 7 assigned to that image, and this\n\n14\n00:01:29,560 --> 00:01:32,780\ndog here might have a number 2 assigned to the image.\n\n15\n00:01:33,140 --> 00:01:37,540\nYou may have heard the saying \"birds of a feather flock together\" but have you also\n\n16\n00:01:37,720 --> 00:01:41,280\nheard the saying \"dogs of the same fur frolic together\"?\n\n17\n00:01:41,580 --> 00:01:43,560\nProbably not, because I just made that up.\n\n18\n00:01:43,980 --> 00:01:48,420\nBut I hope this helps you remember that if you take more images of dogs and try to place\n\n19\n00:01:48,440 --> 00:01:53,500\nthem in the space defined by these two number lines, then you might have a dog here on the bottom left,\n\n20\n00:01:53,700 --> 00:01:58,320\nmaybe this dog will be placed here on the right, and this dog here might appear somewhere on the\n\n21\n00:01:58,480 --> 00:02:04,040\nbottom left, and this dog might appear over here on the bottom right. Notice that dogs with a similar\n\n22\n00:02:04,240 --> 00:02:10,979\namount of brown fur appear closer to each other, and dogs with white fur also appear closer to each\n\n23\n00:02:11,140 --> 00:02:16,980\nother in this space. Now in practice, we don't as humans choose these specific features for these\n\n24\n00:02:17,000 --> 00:02:21,880\nnumber lines. Instead, we have machine learning models do that for us, because then we get better\n\n25\n00:02:22,140 --> 00:02:27,160\nresults. Whenever you train a machine learning model to perform a task on an image, you get a\n\n26\n00:02:27,320 --> 00:02:32,400\nfairly useful byproduct from this. For example, if you have an image classification model, you can\n\n27\n00:02:32,600 --> 00:02:39,000\ngive it an input like this, and it outputs a class such as the text \"cute dog\". In order for the model\n\n28\n00:02:39,180 --> 00:02:44,739\nto perform this image classification task, it creates a representation of the important features\n\n29\n00:02:44,800 --> 00:02:50,640\nthat help it to decide that this is a cute dog. This representation is actually a list of numbers,\n\n30\n00:02:51,120 --> 00:02:56,360\nmaybe a few hundred to a few thousand numbers. And this is still pretty small if you consider that a\n\n31\n00:02:56,640 --> 00:03:02,520\nphotograph may contain 9 million pixels, which are 9 million numbers times 3 because there's red,\n\n32\n00:03:02,720 --> 00:03:07,840\ngreen, and blue for every pixel. This list of numbers that a model creates in order to represent\n\n33\n00:03:08,100 --> 00:03:14,720\nthe image in a compact way is called an embedding. So if you use those embeddings and treat them like\n\n34\n00:03:14,740 --> 00:03:19,660\ncoordinates in space, you can place each image somewhere in that space. Here, I'm just showing\n\n35\n00:03:19,900 --> 00:03:24,740\na two-dimensional space so that we can see it on a two-dimensional slide. So it would just be two\n\n36\n00:03:24,980 --> 00:03:29,760\nnumbers representing an image. But with embeddings of 200 to 300 numbers, the math is similar.\n\n37\n00:03:30,140 --> 00:03:35,000\nIf you draw a line from the location of this image to the location of this other image,\n\n38\n00:03:35,340 --> 00:03:40,580\nthis line shows how far apart those two images are. If you draw a line between these two images,\n\n39\n00:03:40,860 --> 00:03:46,600\nthat line also shows how far apart they are. And in this case, they're further apart compared to\n\n40\n00:03:46,700 --> 00:03:51,540\nthe first line. You can measure the distance between any two images as the distance between\n\n41\n00:03:51,860 --> 00:03:57,140\ntheir two embeddings. And this class is more focused on getting a conceptual understanding\n\n42\n00:03:57,500 --> 00:04:02,880\nas well as practical use of AI. So I'm not getting into the math here, but it's probably math that\n\n43\n00:04:02,980 --> 00:04:08,079\nyou've seen at some point in school. And since you may or may not want to relive your memories of\n\n44\n00:04:08,100 --> 00:04:13,960\nmath class, I won't get into it here. So let's just clarify the difference between image classification\n\n45\n00:04:14,740 --> 00:04:20,040\nand similarity search with images. With image classification, if you give it the image of a dog,\n\n46\n00:04:20,280 --> 00:04:26,520\nit will output some class, such as the text cute dog. In contrast, if you give an image to a\n\n47\n00:04:26,860 --> 00:04:31,780\nsimilarity search database, then it will first create an embedding that represents that image\n\n48\n00:04:32,160 --> 00:04:36,680\nwith a small list of numbers. And then it can search through a database of other dogs, where\n\n49\n00:04:36,700 --> 00:04:41,720\neach dog in the database has an embedding generated to represent it. Based on the distance\n\n50\n00:04:42,360 --> 00:04:47,380\nbetween the embedding of the input dog and the embedding of each dog in the database,\n\n51\n00:04:48,100 --> 00:04:53,480\nthe similarity search database can retrieve a list of dogs that have the shortest distance\n\n52\n00:04:54,120 --> 00:05:00,320\nbetween them and the dog being searched. So here's a quick quiz. Who gave the dog a name?\n\n53\n00:05:00,840 --> 00:05:05,940\nthe image classifier, and who let the dogs out, the similarity search database.\n\n54\n00:05:06,760 --> 00:05:12,500\nNext, what if you want to go beyond a bounding box and identify every single pixel that belongs\n\n55\n00:05:12,700 --> 00:05:15,980\nto this dog, or every pixel that belongs to this dog?\n\n56\n00:05:16,400 --> 00:05:20,840\nTo learn more about how, let's take a look at image segmentation in the next video.\n\n": "1\n00:00:05,139 --> 00:00:09,960\nLorsque vous utilisez un service de d\u00e9tection d'objets tel que Google Lens, vous remarquerez que ci-dessous\n\n2\n00:00:10,199 --> 00:00:14,699\nles classes sugg\u00e9r\u00e9es par le classificateur d'images, vous obtiendrez des r\u00e9sultats de recherche pour des images similaires.\n\n3\n00:00:15,380 --> 00:00:21,039\nCes chiens blancs \u00e0 poils longs ressemblent donc au chien blanc \u00e0 poils longs d'origine. De m\u00eame,\n\n4\n00:00:21,539 --> 00:00:26,960\napr\u00e8s les classes propos\u00e9es pour ce chien marron mignon, vous pouvez voir des images similaires d'autres chiens mignons.\n\n5\n00:00:26,980 --> 00:00:33,320\nbrown dogs. This is called similarity search, and you can apply this to finding similar images and\n\n6\n00:00:33,500 --> 00:00:39,679\n\u00e9galement \u00e0 la recherche d'un son ou d'un texte similaire. Voici un bref aper\u00e7u conceptuel de la fa\u00e7on dont les\n\n7\n00:00:40,020 --> 00:00:46,359\nla recherche par similarit\u00e9 fonctionne. Que se passerait-il si vous essayiez d'utiliser un seul chiffre pour repr\u00e9senter n'importe quelle image afin de\n\n8\n00:00:46,500 --> 00:00:56,939\nvous aide \u00e0 comparer et \u00e0 opposer deux images ? Choisissons une caract\u00e9ristique et tra\u00e7ons une ligne num\u00e9rique. Pour\n\n9\n00:00:56,960 --> 00:01:03,799\nSo for this dog, you might assign it a number 1, whereas for this dog, you might assign it a number 6.\n\n10\n00:01:04,219 --> 00:01:11,060\nVous donnez donc \u00e0 ce chien un chiffre plus \u00e9lev\u00e9 parce que, sur l'image, il semble avoir une fourrure plus brune que l'autre chien.\n\n11\n00:01:11,579 --> 00:01:15,239\nNow what if you can represent the image with not one, but two numbers?\n\n12\n00:01:15,960 --> 00:01:23,299\nAinsi, en plus de la quantit\u00e9 de fourrure brune, vous pouvez tracer une autre ligne num\u00e9rique pour repr\u00e9senter la quantit\u00e9 de fourrure blanche.\n\n13\n00:01:23,780 --> 00:01:29,359\nAinsi, le num\u00e9ro 7 pourrait \u00eatre attribu\u00e9 \u00e0 l'image du premier chien, et ce num\u00e9ro pourrait \u00eatre attribu\u00e9 \u00e0 l'image du deuxi\u00e8me chien.\n\n14\n00:01:29,560 --> 00:01:32,780\ndog here might have a number 2 assigned to the image.\n\n15\n00:01:33,140 --> 00:01:37,539\nVous avez peut-\u00eatre d\u00e9j\u00e0 entendu l'expression \"les oiseaux qui se ressemblent s'assemblent\", mais avez-vous d\u00e9j\u00e0 entendu l'expression \"les oiseaux qui se ressemblent s'assemblent\" ?\n\n16\n00:01:37,719 --> 00:01:41,280\nheard the saying \"dogs of the same fur frolic together\"?\n\n17\n00:01:41,579 --> 00:01:43,560\nProbablement pas, car je viens de l'inventer.\n\n18\n00:01:43,979 --> 00:01:48,420\nMais j'esp\u00e8re que cela vous aidera \u00e0 vous rappeler que si vous prenez plus d'images de chiens et que vous essayez de les placer dans un contexte plus large, il est possible de les placer dans un contexte plus large.\n\n19\n00:01:48,439 --> 00:01:53,500\nthem in the space defined by these two number lines, then you might have a dog here on the bottom left,\n\n20\n00:01:53,700 --> 00:01:58,319\npeut-\u00eatre que ce chien sera plac\u00e9 ici \u00e0 droite, et que ce chien ici appara\u00eetra quelque part sur la droite.\n\n21\n00:01:58,479 --> 00:02:04,040\nen bas \u00e0 gauche, et ce chien pourrait appara\u00eetre ici en bas \u00e0 droite. Remarquez que les chiens ayant une\n\n22\n00:02:04,239 --> 00:02:10,978\namount of brown fur appear closer to each other, and dogs with white fur also appear closer to each\n\n23\n00:02:11,139 --> 00:02:16,979\ndans cet espace. Dans la pratique, nous ne choisissons pas, en tant qu'humains, ces caract\u00e9ristiques sp\u00e9cifiques pour ces\n\n24\n00:02:17,000 --> 00:02:21,879\nnumber lines. Instead, we have machine learning models do that for us, because then we get better\n\n25\n00:02:22,139 --> 00:02:27,159\nresults. Whenever you train a machine learning model to perform a task on an image, you get a\n\n26\n00:02:27,319 --> 00:02:32,400\nfairly useful byproduct from this. For example, if you have an image classification model, you can\n\n27\n00:02:32,599 --> 00:02:39,000\ngive it an input like this, and it outputs a class such as the text \"cute dog\". In order for the model\n\n28\n00:02:39,180 --> 00:02:44,739\nto perform this image classification task, it creates a representation of the important features\n\n29\n00:02:44,800 --> 00:02:50,639\nthat help it to decide that this is a cute dog. This representation is actually a list of numbers,\n\n30\n00:02:51,120 --> 00:02:56,360\npeut-\u00eatre quelques centaines ou quelques milliers de num\u00e9ros. Et c'est encore assez peu si l'on consid\u00e8re qu'un\n\n31\n00:02:56,639 --> 00:03:02,520\nphotograph may contain 9 million pixels, which are 9 million numbers times 3 because there's red,\n\n32\n00:03:02,719 --> 00:03:07,840\nvert et bleu pour chaque pixel. Cette liste de nombres qu'un mod\u00e8le cr\u00e9e pour repr\u00e9senter les\n\n33\n00:03:08,099 --> 00:03:14,719\nthe image in a compact way is called an embedding. So if you use those embeddings and treat them like\n\n34\n00:03:14,740 --> 00:03:19,659\ncoordinates in space, you can place each image somewhere in that space. Here, I'm just showing\n\n35\n00:03:19,900 --> 00:03:24,740\nun espace bidimensionnel afin que nous puissions le voir sur une diapositive bidimensionnelle. Il s'agirait donc simplement de deux\n\n36\n00:03:24,979 --> 00:03:29,759\nnumbers representing an image. But with embeddings of 200 to 300 numbers, the math is similar.\n\n37\n00:03:30,139 --> 00:03:35,000\nIf you draw a line from the location of this image to the location of this other image,\n\n38\n00:03:35,340 --> 00:03:40,579\nthis line shows how far apart those two images are. If you draw a line between these two images,\n\n39\n00:03:40,860 --> 00:03:46,599\ncette ligne indique \u00e9galement la distance qui les s\u00e9pare. Et dans ce cas, ils sont plus \u00e9loign\u00e9s les uns des autres par rapport \u00e0\n\n40\n00:03:46,699 --> 00:03:51,539\nthe first line. You can measure the distance between any two images as the distance between\n\n41\n00:03:51,860 --> 00:03:57,139\ntheir two embeddings. And this class is more focused on getting a conceptual understanding\n\n42\n00:03:57,500 --> 00:04:02,879\nas well as practical use of AI. So I'm not getting into the math here, but it's probably math that\n\n43\n00:04:02,979 --> 00:04:08,079\nque vous avez vus \u00e0 un moment ou \u00e0 un autre de votre scolarit\u00e9. Et puisque vous voulez ou ne voulez pas revivre vos souvenirs de\n\n44\n00:04:08,099 --> 00:04:13,960\nmath class, I won't get into it here. So let's just clarify the difference between image classification\n\n45\n00:04:14,740 --> 00:04:20,040\nand similarity search with images. With image classification, if you give it the image of a dog,\n\n46\n00:04:20,279 --> 00:04:26,519\nil produira une certaine classe, comme le texte chien mignon. En revanche, si vous donnez une image \u00e0 un fichier\n\n47\n00:04:26,860 --> 00:04:31,779\nla base de donn\u00e9es de recherche de similarit\u00e9s, il cr\u00e9era d'abord une int\u00e9gration qui repr\u00e9sente cette image.\n\n48\n00:04:32,159 --> 00:04:36,680\nwith a small list of numbers. And then it can search through a database of other dogs, where\n\n49\n00:04:36,699 --> 00:04:41,720\nchaque chien de la base de donn\u00e9es est repr\u00e9sent\u00e9 par un encastrement. En fonction de la distance\n\n50\n00:04:42,360 --> 00:04:47,379\nbetween the embedding of the input dog and the embedding of each dog in the database,\n\n51\n00:04:48,100 --> 00:04:53,480\nthe similarity search database can retrieve a list of dogs that have the shortest distance\n\n52\n00:04:54,120 --> 00:05:00,319\nbetween them and the dog being searched. So here's a quick quiz. Who gave the dog a name?\n\n53\n00:05:00,839 --> 00:05:05,939\nle classificateur d'images, et qui a laiss\u00e9 sortir les chiens, la base de donn\u00e9es de recherche de similitudes.\n\n54\n00:05:06,759 --> 00:05:12,500\nNext, what if you want to go beyond a bounding box and identify every single pixel that belongs\n\n55\n00:05:12,699 --> 00:05:15,980\n\u00e0 ce chien, ou chaque pixel appartenant \u00e0 ce chien ?\n\n56\n00:05:16,399 --> 00:05:20,839\nPour en savoir plus, examinons la segmentation des images dans la prochaine vid\u00e9o.\n\n", "1\n00:00:05,260 --> 00:00:09,340\nIf you have any friends who are into photography, they may love to tell you how they can use their\n\n2\n00:00:09,360 --> 00:00:14,140\nprofessional camera to blur the background in photos. If you don't have a professional camera\n\n3\n00:00:14,300 --> 00:00:19,000\nyourself, you can still do that with your regular phone and AI. Let's see how that works with\n\n4\n00:00:19,140 --> 00:00:24,640\nsomething called image segmentation. In most photo apps, you can create a portrait effect\n\n5\n00:00:25,060 --> 00:00:31,060\nby blurring the background. For example, in Google Photos, you can click on Edit and then click on\n\n6\n00:00:31,240 --> 00:00:37,120\nPortrait. Notice that it's blurring the image, but not the whole image, just the background behind me.\n\n7\n00:00:37,500 --> 00:00:42,500\nSo again, here is the original image, and here is the image with the blurred background. This is\n\n8\n00:00:42,720 --> 00:00:47,540\ncalled the Bokeh effect, and people like that it helps the viewer focus their attention on the\n\n9\n00:00:47,900 --> 00:00:52,900\nsubject of the photo. And don't worry, Bokeh effect is not an AI-related word that you need to\n\n10\n00:00:52,920 --> 00:00:58,200\nremember. Photographers love talking about the bokeh effect, so it's just a nice word to know\n\n11\n00:00:58,600 --> 00:01:04,720\nin case you're friends with a photographer. The photo app is able to identify specifically which\n\n12\n00:01:04,920 --> 00:01:09,460\nparts of an image represent the person in the foreground as opposed to everything else in the\n\n13\n00:01:09,580 --> 00:01:16,740\nbackground on a pixel by pixel level. And by pixel, I'm referring to all the tiny little dots that\n\n14\n00:01:16,920 --> 00:01:21,860\nmake up a digital photograph. Here's what I mean. If you look at just a small section of the\n\n15\n00:01:21,880 --> 00:01:27,260\nphotograph and then zoom in. Imagine if we zoomed in enough that we're actually looking at all the\n\n16\n00:01:27,580 --> 00:01:32,820\npixels that make up an image. It's essentially like a spreadsheet where every cell of the spreadsheet\n\n17\n00:01:33,160 --> 00:01:38,540\nhas a number that defines how much red, how much green, and how much blue is in that particular\n\n18\n00:01:38,860 --> 00:01:44,340\npixel. And in order to generate this portrait photo effect, the software attempts to identify\n\n19\n00:01:44,840 --> 00:01:51,840\nfor every pixel whether that pixel is part of the object in the foreground, or\n\n20\n00:01:51,860 --> 00:01:56,400\nor is part of the background.\n\n21\n00:02:01,880 --> 00:02:07,620\nSo, the ability to determine which pixel represents the person and which pixel represents the\n\n22\n00:02:07,780 --> 00:02:11,740\nbackground is a computer vision task called image segmentation.\n\n23\n00:02:12,240 --> 00:02:17,280\nNote, I am skipping over some additional processing that goes in in order to identify the foreground\n\n24\n00:02:17,300 --> 00:02:22,500\nversus the background, and this is called depth perception, but it is possible to just rely on\n\n25\n00:02:22,720 --> 00:02:27,180\nimage segmentation to create this portrait effect, so for now don't worry about remembering what\n\n26\n00:02:27,440 --> 00:02:32,000\ndepth perception is. The output of image segmentation is actually going to be something\n\n27\n00:02:32,300 --> 00:02:37,280\nthat looks like an image with the same height and width of the original photograph, but where each\n\n28\n00:02:37,440 --> 00:02:44,500\npixel is given a value of 0 or 1. In this case, I'm representing the 1 values with the color white\n\n29\n00:02:44,720 --> 00:02:49,560\nand the zero values with the color black. But these can be any color for visualization purposes.\n\n30\n00:02:50,020 --> 00:02:56,000\nThe computer itself just sees a spreadsheet-like grid of zeros and ones. So a value of one indicates\n\n31\n00:02:56,280 --> 00:03:01,180\nthat a pixel from the original image is part of the segmented object, in this case the person.\n\n32\n00:03:01,700 --> 00:03:06,560\nA value of zero represents anything that was not part of the segmented object. In this case,\n\n33\n00:03:06,820 --> 00:03:12,760\nthat's the background. This image of just zeros and ones is called a mask. You can think of it the\n\n34\n00:03:12,780 --> 00:03:17,740\nsame way you think of a mask you wear for a costume. It outlines particular areas of the image,\n\n35\n00:03:18,020 --> 00:03:22,380\njust like a mask that you might wear if you're a superhero or just attending some dress-up event\n\n36\n00:03:22,600 --> 00:03:28,580\noutlines a certain area of your face. And this mask can then be used downstream for other steps\n\n37\n00:03:28,840 --> 00:03:35,200\nin the software application to process just the pixels that are the person or just the pixels that\n\n38\n00:03:35,300 --> 00:03:40,020\nare in the background. So here's what you can do to get the blur effect. So here's the original image,\n\n39\n00:03:40,420 --> 00:03:45,580\nAnd here is the mask. And here is where we want to store just the pixels of the person in the\n\n40\n00:03:45,700 --> 00:03:51,700\nforeground. For this pixel, let's say the value is 125. And in the mask, the value at that same\n\n41\n00:03:51,940 --> 00:04:01,520\nlocation is 0. So 125 times 0 is 0. And let's put a 0 here. Let's try another pixel here. Let's say\n\n42\n00:04:01,600 --> 00:04:10,720\nit's 150. In the mask, the value at the same location is 1. 150 times 1 is 150. So let's save\n\n43\n00:04:10,840 --> 00:04:15,680\nthat here. And if you do that for all the pixels, you'll get an image that has the same pixels of\n\n44\n00:04:15,780 --> 00:04:20,799\nthe subject, like this. And I'm simplifying a little bit for the sake of clarity. Normally,\n\n45\n00:04:21,200 --> 00:04:26,520\nfor color photos, every pixel stores three values. One for how much red, how much green,\n\n46\n00:04:26,980 --> 00:04:31,880\nand how much blue, and that mixes together to produce the color. The next step is to go back\n\n47\n00:04:32,040 --> 00:04:37,080\nto the original image and blur the whole thing. Yes, the background and even the subject in the\n\n48\n00:04:37,260 --> 00:04:42,660\nforeground. Everything. I'll explain why in a bit. Then, take this subject that you carved out using\n\n49\n00:04:42,920 --> 00:04:47,960\nthe segmentation mask and overlay it on top of the blurred image. And that's how you get the nice\n\n50\n00:04:48,160 --> 00:04:53,699\nportrait bokeh effect with a blurred background using image segmentation. Now, you may be wondering\n\n51\n00:04:53,720 --> 00:04:59,160\nwhy we take this somewhat indirect approach of getting the mask of the subject, blurring the\n\n52\n00:04:59,540 --> 00:05:05,440\nentire photo, and then adding the subject back on top. For instance, what if you just used the\n\n53\n00:05:05,640 --> 00:05:11,140\nimage segmentation mask to identify all the pixels of the background behind the subject and just\n\n54\n00:05:11,440 --> 00:05:15,660\ndirectly blur those pixels? Well, let's zoom in a bit to see what might happen. The thing is,\n\n55\n00:05:15,940 --> 00:05:20,539\nthe way the blurring effect works is kind of like dipping your finger in water and then moving it\n\n56\n00:05:20,560 --> 00:05:23,340\nover a watercolor painting to get the pixels to mix together.\n\n57\n00:05:23,940 --> 00:05:26,740\nSo if you apply the blur somewhere here, that's fine.\n\n58\n00:05:27,140 --> 00:05:29,640\nBut when you try to blur the parts of the background\n\n59\n00:05:30,040 --> 00:05:32,920\nthat border the subject, which you don't want to be blurry,\n\n60\n00:05:33,320 --> 00:05:35,780\nthen you'll end up blurring the pixels of the subject as well.\n\n61\n00:05:36,160 --> 00:05:38,840\nSo if you do the recommended method that you saw earlier,\n\n62\n00:05:39,160 --> 00:05:41,520\nthis helps you to make sure that the person in the photo\n\n63\n00:05:41,820 --> 00:05:43,560\ndoesn't have a blurry outline around them.\n\n64\n00:05:45,920 --> 00:05:46,800\nOh, and one more thing.\n\n65\n00:05:47,320 --> 00:05:49,679\nYou can actually try out image segmentation yourself\n\n66\n00:05:49,700 --> 00:05:50,920\nby going to Meta AI's website.\n\n67\n00:05:51,640 --> 00:05:53,940\nIf you start from the Meta AI chatbot site,\n\n68\n00:05:54,320 --> 00:05:55,720\nyou can click on the left-hand menu\n\n69\n00:05:56,120 --> 00:05:57,860\nand then click on AI Demos.\n\n70\n00:05:58,280 --> 00:06:01,820\nThe AI Demos page includes a section called Segment Anything.\n\n71\n00:06:02,260 --> 00:06:03,300\nIt's currently version two\n\n72\n00:06:03,560 --> 00:06:04,580\nat the time that I'm filming this.\n\n73\n00:06:05,040 --> 00:06:05,760\nSo let's click on that.\n\n74\n00:06:06,140 --> 00:06:07,740\nThen I'll click on the button, Try It Now.\n\n75\n00:06:08,100 --> 00:06:09,460\nIt starts out with a default video\n\n76\n00:06:09,800 --> 00:06:10,980\nthat's about 10 seconds long,\n\n77\n00:06:11,280 --> 00:06:12,760\nbut I'll go here and click on the button\n\n78\n00:06:12,920 --> 00:06:14,120\nthat says Change Video.\n\n79\n00:06:14,620 --> 00:06:16,240\nAnd there are other videos to choose from,\n\n80\n00:06:16,560 --> 00:06:18,620\nbut I'm going to drag and drop a video of my own.\n\n81\n00:06:18,940 --> 00:06:23,500\nNote that for any video that you upload, it will show about the first 9 to 10 seconds of it.\n\n82\n00:06:23,860 --> 00:06:26,260\nSo I'll go ahead and click play and take a look at the video.\n\n83\n00:06:26,700 --> 00:06:30,180\nNow I'm going to pause the video so that I can click on an object to segment.\n\n84\n00:06:30,580 --> 00:06:33,220\nAnd in this case, I'll click on this emoji ball in my hand.\n\n85\n00:06:33,440 --> 00:06:37,920\nIt gives me the option to add more mouse clicks to define the object that I'm interested in.\n\n86\n00:06:38,180 --> 00:06:42,800\nBut this looks pretty good, so I'll just go ahead and click on the button here that says track objects.\n\n87\n00:06:43,240 --> 00:06:45,700\nAfter a few seconds, it starts to play the video again.\n\n88\n00:06:46,100 --> 00:06:49,480\nAnd you can see that the emoji ball is being outlined by a blue line.\n\n89\n00:06:49,860 --> 00:06:51,340\nSo if you click on the button that says next,\n\n90\n00:06:51,640 --> 00:06:55,440\nit gives you some options for things that you can do with the object that you've now segmented.\n\n91\n00:06:55,760 --> 00:06:59,080\nFor instance, you can click on erase and then gradient.\n\n92\n00:06:59,760 --> 00:07:01,240\nAnd you can also click on pixelate.\n\n93\n00:07:01,520 --> 00:07:04,380\nIf you click on spotlight, it looks like it's outlining the emoji ball.\n\n94\n00:07:04,720 --> 00:07:06,540\nIf you click on burst, it looks like this.\n\n95\n00:07:06,960 --> 00:07:08,360\nAnd you can actually click on emoji.\n\n96\n00:07:08,900 --> 00:07:14,720\nSo in this video, it's actually replacing this emoji ball with an actual emoji.\n\n97\n00:07:15,160 --> 00:07:17,300\nand you can cycle through a couple different emojis.\n\n98\n00:07:17,820 --> 00:07:18,320\nThat's pretty cool.\n\n99\n00:07:18,680 --> 00:07:19,940\nYou can also update the background,\n\n100\n00:07:20,620 --> 00:07:23,740\nand it would only affect the background and not the objects that are segmented.\n\n101\n00:07:24,020 --> 00:07:25,620\nSo I'd encourage you to try it out for yourself.\n\n102\n00:07:26,020 --> 00:07:30,180\nMaybe you can find a fun video that you'd like to use to practice image segmentation.\n\n103\n00:07:30,540 --> 00:07:31,200\nComing up next,\n\n104\n00:07:31,620 --> 00:07:34,080\nwhat if you took a picture of something that has text in it,\n\n105\n00:07:34,280 --> 00:07:36,620\nlike an outdoor sign or a restaurant receipt,\n\n106\n00:07:37,040 --> 00:07:40,500\nand wanted to be able to copy that text into a text message, for instance?\n\n107\n00:07:41,040 --> 00:07:42,480\nLet's look at how to do that next.\n\n": "1\n00:00:05,259 --> 00:00:09,339\nSi vous avez des amis qui aiment la photographie, ils seront ravis de vous expliquer comment ils peuvent utiliser leur appareil photo.\n\n2\n00:00:09,359 --> 00:00:14,140\nappareil photo professionnel pour rendre l'arri\u00e8re-plan flou sur les photos. Si vous ne disposez pas d'un appareil photo professionnel\n\n3\n00:00:14,300 --> 00:00:19,000\nyourself, you can still do that with your regular phone and AI. Let's see how that works with\n\n4\n00:00:19,140 --> 00:00:24,640\nce que l'on appelle la segmentation d'image. Dans la plupart des applications photo, vous pouvez cr\u00e9er un effet de portrait\n\n5\n00:00:25,059 --> 00:00:31,059\nby blurring the background. For example, in Google Photos, you can click on Edit and then click on\n\n6\n00:00:31,239 --> 00:00:37,119\nPortrait. Remarquez que l'image est floue, mais pas toute l'image, seulement l'arri\u00e8re-plan derri\u00e8re moi.\n\n7\n00:00:37,500 --> 00:00:42,500\nSo again, here is the original image, and here is the image with the blurred background. This is\n\n8\n00:00:42,719 --> 00:00:47,539\ncalled the Bokeh effect, and people like that it helps the viewer focus their attention on the\n\n9\n00:00:47,899 --> 00:00:52,899\nsujet de la photo. Et ne vous inqui\u00e9tez pas, l'effet Bokeh n'est pas un terme li\u00e9 \u00e0 l'IA que vous devez conna\u00eetre.\n\n10\n00:00:52,920 --> 00:00:58,200\nse souvenir. Les photographes adorent parler de l'effet bokeh, c'est donc un mot agr\u00e9able \u00e0 conna\u00eetre\n\n11\n00:00:58,600 --> 00:01:04,719\nsi vous \u00eates ami avec un photographe. L'application photo est capable d'identifier pr\u00e9cis\u00e9ment les\n\n12\n00:01:04,920 --> 00:01:09,460\nLes parties d'une image repr\u00e9sentent la personne au premier plan par opposition \u00e0 tout le reste de l'image.\n\n13\n00:01:09,579 --> 00:01:16,739\nbackground on a pixel by pixel level. And by pixel, I'm referring to all the tiny little dots that\n\n14\n00:01:16,920 --> 00:01:21,859\ncomposent une photographie num\u00e9rique. Voici ce que je veux dire. Si vous regardez juste une petite partie de l'image, vous verrez qu'il y a une grande diff\u00e9rence entre les deux.\n\n15\n00:01:21,879 --> 00:01:27,260\nphotograph and then zoom in. Imagine if we zoomed in enough that we're actually looking at all the\n\n16\n00:01:27,579 --> 00:01:32,819\nles pixels qui composent une image. C'est essentiellement comme une feuille de calcul dont chaque cellule\n\n17\n00:01:33,159 --> 00:01:38,539\ncomporte un nombre qui d\u00e9finit la quantit\u00e9 de rouge, de vert et de bleu dans cette\n\n18\n00:01:38,859 --> 00:01:44,340\npixel. And in order to generate this portrait photo effect, the software attempts to identify\n\n19\n00:01:44,840 --> 00:01:51,840\npour chaque pixel, si ce pixel fait partie de l'objet au premier plan, ou\n\n20\n00:01:51,859 --> 00:01:56,400\nor is part of the background.\n\n21\n00:02:01,879 --> 00:02:07,620\nAinsi, la capacit\u00e9 de d\u00e9terminer quel pixel repr\u00e9sente la personne et quel pixel repr\u00e9sente la\n\n22\n00:02:07,780 --> 00:02:11,740\nbackground is a computer vision task called image segmentation.\n\n23\n00:02:12,240 --> 00:02:17,280\nNote, I am skipping over some additional processing that goes in in order to identify the foreground\n\n24\n00:02:17,300 --> 00:02:22,500\nversus the background, and this is called depth perception, but it is possible to just rely on\n\n25\n00:02:22,719 --> 00:02:27,180\nsegmentation d'image pour cr\u00e9er cet effet de portrait, donc pour l'instant ne vous souciez pas de vous souvenir de ce que vous avez fait.\n\n26\n00:02:27,439 --> 00:02:32,000\ndepth perception is. The output of image segmentation is actually going to be something\n\n27\n00:02:32,300 --> 00:02:37,280\nthat looks like an image with the same height and width of the original photograph, but where each\n\n28\n00:02:37,439 --> 00:02:44,500\npixel is given a value of 0 or 1. In this case, I'm representing the 1 values with the color white\n\n29\n00:02:44,719 --> 00:02:49,560\net les valeurs z\u00e9ro avec la couleur noire. Mais celles-ci peuvent \u00eatre de n'importe quelle couleur \u00e0 des fins de visualisation.\n\n30\n00:02:50,020 --> 00:02:56,000\nThe computer itself just sees a spreadsheet-like grid of zeros and ones. So a value of one indicates\n\n31\n00:02:56,280 --> 00:03:01,180\nthat a pixel from the original image is part of the segmented object, in this case the person.\n\n32\n00:03:01,699 --> 00:03:06,560\nA value of zero represents anything that was not part of the segmented object. In this case,\n\n33\n00:03:06,819 --> 00:03:12,759\nthat's the background. This image of just zeros and ones is called a mask. You can think of it the\n\n34\n00:03:12,780 --> 00:03:17,740\nIl s'agit de la m\u00eame chose qu'un masque que l'on porte pour se d\u00e9guiser. Il d\u00e9limite des zones particuli\u00e8res de l'image,\n\n35\n00:03:18,020 --> 00:03:22,379\njust like a mask that you might wear if you're a superhero or just attending some dress-up event\n\n36\n00:03:22,599 --> 00:03:28,579\noutlines a certain area of your face. And this mask can then be used downstream for other steps\n\n37\n00:03:28,840 --> 00:03:35,199\nin the software application to process just the pixels that are the person or just the pixels that\n\n38\n00:03:35,300 --> 00:03:40,020\nse trouvent \u00e0 l'arri\u00e8re-plan. Voici donc ce que vous pouvez faire pour obtenir l'effet de flou. Voici l'image originale,\n\n39\n00:03:40,420 --> 00:03:45,579\nAnd here is the mask. And here is where we want to store just the pixels of the person in the\n\n40\n00:03:45,699 --> 00:03:51,699\nforeground. For this pixel, let's say the value is 125. And in the mask, the value at that same\n\n41\n00:03:51,939 --> 00:04:01,520\nlocation is 0. So 125 times 0 is 0. And let's put a 0 here. Let's try another pixel here. Let's say\n\n42\n00:04:01,599 --> 00:04:10,719\nit's 150. In the mask, the value at the same location is 1. 150 times 1 is 150. So let's save\n\n43\n00:04:10,840 --> 00:04:15,680\nthat here. And if you do that for all the pixels, you'll get an image that has the same pixels of\n\n44\n00:04:15,780 --> 00:04:20,798\nthe subject, like this. And I'm simplifying a little bit for the sake of clarity. Normally,\n\n45\n00:04:21,199 --> 00:04:26,519\nfor color photos, every pixel stores three values. One for how much red, how much green,\n\n46\n00:04:26,980 --> 00:04:31,879\net combien de bleu, qui se m\u00e9langent pour produire la couleur. L'\u00e9tape suivante consiste \u00e0 revenir en arri\u00e8re\n\n47\n00:04:32,040 --> 00:04:37,079\nto the original image and blur the whole thing. Yes, the background and even the subject in the\n\n48\n00:04:37,259 --> 00:04:42,659\nforeground. Everything. I'll explain why in a bit. Then, take this subject that you carved out using\n\n49\n00:04:42,920 --> 00:04:47,959\nthe segmentation mask and overlay it on top of the blurred image. And that's how you get the nice\n\n50\n00:04:48,159 --> 00:04:53,699\nportrait bokeh effect with a blurred background using image segmentation. Now, you may be wondering\n\n51\n00:04:53,720 --> 00:04:59,159\nwhy we take this somewhat indirect approach of getting the mask of the subject, blurring the\n\n52\n00:04:59,540 --> 00:05:05,439\nde l'ensemble de la photo, puis en rajoutant le sujet. Par exemple, si vous n'utilisiez que la fonction\n\n53\n00:05:05,639 --> 00:05:11,139\nmasque de segmentation d'image pour identifier tous les pixels de l'arri\u00e8re-plan derri\u00e8re le sujet et seulement les pixels de l'arri\u00e8re-plan derri\u00e8re le sujet et les pixels de l'arri\u00e8re-plan derri\u00e8re le sujet.\n\n54\n00:05:11,439 --> 00:05:15,660\ndirectly blur those pixels? Well, let's zoom in a bit to see what might happen. The thing is,\n\n55\n00:05:15,939 --> 00:05:20,538\nthe way the blurring effect works is kind of like dipping your finger in water and then moving it\n\n56\n00:05:20,560 --> 00:05:23,339\nover a watercolor painting to get the pixels to mix together.\n\n57\n00:05:23,939 --> 00:05:26,740\nSo if you apply the blur somewhere here, that's fine.\n\n58\n00:05:27,139 --> 00:05:29,639\nMais lorsque vous essayez d'estomper les parties de l'arri\u00e8re-plan\n\n59\n00:05:30,040 --> 00:05:32,920\nqui bordent le sujet, que vous ne voulez pas voir flou,\n\n60\n00:05:33,319 --> 00:05:35,779\nthen you'll end up blurring the pixels of the subject as well.\n\n61\n00:05:36,159 --> 00:05:38,840\nSo if you do the recommended method that you saw earlier,\n\n62\n00:05:39,159 --> 00:05:41,519\nthis helps you to make sure that the person in the photo\n\n63\n00:05:41,819 --> 00:05:43,560\ndoesn't have a blurry outline around them.\n\n64\n00:05:45,920 --> 00:05:46,800\nOh, et encore une chose.\n\n65\n00:05:47,319 --> 00:05:49,678\nVous pouvez tester vous-m\u00eame la segmentation d'images\n\n66\n00:05:49,699 --> 00:05:50,920\nby going to Meta AI's website.\n\n67\n00:05:51,639 --> 00:05:53,939\nSi vous partez du site du chatbot Meta AI,\n\n68\n00:05:54,319 --> 00:05:55,720\nyou can click on the left-hand menu\n\n69\n00:05:56,120 --> 00:05:57,860\nand then click on AI Demos.\n\n70\n00:05:58,279 --> 00:06:01,819\nThe AI Demos page includes a section called Segment Anything.\n\n71\n00:06:02,259 --> 00:06:03,300\nIl s'agit actuellement de la deuxi\u00e8me version\n\n72\n00:06:03,560 --> 00:06:04,579\nat the time that I'm filming this.\n\n73\n00:06:05,040 --> 00:06:05,759\nCliquez donc sur ce point.\n\n74\n00:06:06,139 --> 00:06:07,740\nJe clique ensuite sur le bouton \"Essayer maintenant\".\n\n75\n00:06:08,100 --> 00:06:09,459\nIl commence par une vid\u00e9o par d\u00e9faut\n\n76\n00:06:09,800 --> 00:06:10,980\nqui dure environ 10 secondes,\n\n77\n00:06:11,279 --> 00:06:12,759\nbut I'll go here and click on the button\n\n78\n00:06:12,920 --> 00:06:14,120\nthat says Change Video.\n\n79\n00:06:14,620 --> 00:06:16,240\nAnd there are other videos to choose from,\n\n80\n00:06:16,560 --> 00:06:18,620\nmais je vais glisser et d\u00e9poser une vid\u00e9o de mon cru.\n\n81\n00:06:18,939 --> 00:06:23,500\nNotez que pour toute vid\u00e9o que vous t\u00e9l\u00e9chargez, les 9 \u00e0 10 premi\u00e8res secondes seront affich\u00e9es.\n\n82\n00:06:23,860 --> 00:06:26,259\nJe vais donc cliquer sur \"play\" et regarder la vid\u00e9o.\n\n83\n00:06:26,699 --> 00:06:30,180\nNow I'm going to pause the video so that I can click on an object to segment.\n\n84\n00:06:30,579 --> 00:06:33,220\nAnd in this case, I'll click on this emoji ball in my hand.\n\n85\n00:06:33,439 --> 00:06:37,920\nIl me permet d'ajouter d'autres clics de souris pour d\u00e9finir l'objet qui m'int\u00e9resse.\n\n86\n00:06:38,180 --> 00:06:42,800\nMais \u00e7a a l'air plut\u00f4t bien, alors je vais aller de l'avant et cliquer sur le bouton ici qui dit de suivre les objets.\n\n87\n00:06:43,240 --> 00:06:45,699\nAfter a few seconds, it starts to play the video again.\n\n88\n00:06:46,100 --> 00:06:49,480\nAnd you can see that the emoji ball is being outlined by a blue line.\n\n89\n00:06:49,860 --> 00:06:51,340\nSo if you click on the button that says next,\n\n90\n00:06:51,639 --> 00:06:55,439\nit gives you some options for things that you can do with the object that you've now segmented.\n\n91\n00:06:55,759 --> 00:06:59,079\nFor instance, you can click on erase and then gradient.\n\n92\n00:06:59,759 --> 00:07:01,240\nAnd you can also click on pixelate.\n\n93\n00:07:01,519 --> 00:07:04,379\nSi vous cliquez sur le spotlight, on dirait qu'il d\u00e9limite l'emoji ball.\n\n94\n00:07:04,720 --> 00:07:06,540\nIf you click on burst, it looks like this.\n\n95\n00:07:06,959 --> 00:07:08,360\nEt vous pouvez cliquer sur les emoji.\n\n96\n00:07:08,899 --> 00:07:14,720\nSo in this video, it's actually replacing this emoji ball with an actual emoji.\n\n97\n00:07:15,160 --> 00:07:17,300\net vous pouvez passer d'un emoji \u00e0 l'autre.\n\n98\n00:07:17,819 --> 00:07:18,319\nC'est plut\u00f4t cool.\n\n99\n00:07:18,680 --> 00:07:19,939\nVous pouvez \u00e9galement mettre \u00e0 jour l'arri\u00e8re-plan,\n\n100\n00:07:20,620 --> 00:07:23,740\net cela n'affecterait que l'arri\u00e8re-plan et non les objets segment\u00e9s.\n\n101\n00:07:24,019 --> 00:07:25,620\nSo I'd encourage you to try it out for yourself.\n\n102\n00:07:26,019 --> 00:07:30,180\nMaybe you can find a fun video that you'd like to use to practice image segmentation.\n\n103\n00:07:30,540 --> 00:07:31,199\nComing up next,\n\n104\n00:07:31,620 --> 00:07:34,079\nEt si vous preniez une photo de quelque chose qui contient du texte,\n\n105\n00:07:34,279 --> 00:07:36,620\ncomme une enseigne ext\u00e9rieure ou un ticket de restaurant,\n\n106\n00:07:37,040 --> 00:07:40,500\net que vous souhaitiez pouvoir copier ce texte dans un message texte, par exemple ?\n\n107\n00:07:41,040 --> 00:07:42,480\nVoyons maintenant comment proc\u00e9der.\n\n"}