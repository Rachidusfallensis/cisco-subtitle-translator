1
00:00:05,080 --> 00:00:09,360
Say you have a logic problem that you want the chatbot to solve, and you can choose two

2
00:00:09,640 --> 00:00:14,580
slightly different ways to write the last sentence of your prompt. Give your answer,

3
00:00:15,340 --> 00:00:21,180
then give your reasoning. Alternatively, give your reasoning, then give your answer. Here is the logic

4
00:00:21,520 --> 00:00:27,359
problem, and don't worry, I'll talk to the details a bit later, so for now, all you need to know is

5
00:00:27,420 --> 00:00:29,940
is that the correct answer is no.

6
00:00:31,700 --> 00:00:35,020
Now let's see how a chatbot responds when I give it this logic problem

7
00:00:35,640 --> 00:00:38,760
where the last sentence is written in one of these two ways.

8
00:00:39,340 --> 00:00:41,820
So I'll show this text more clearly in a bit,

9
00:00:42,160 --> 00:00:45,580
but on the left, it answers yes, which is incorrect.

10
00:00:46,520 --> 00:00:49,680
On the right, it answers no, which is correct.

11
00:00:50,340 --> 00:00:51,000
Why is that?

12
00:00:51,760 --> 00:00:55,440
The way chatbots produce their output is by predicting the next word,

13
00:00:56,060 --> 00:00:56,600
one at a time.

14
00:00:56,900 --> 00:01:01,240
Let's look at that in more detail and see how you can tailor your prompts to make the

15
00:01:01,400 --> 00:01:02,200
most of this behavior.

16
00:01:02,960 --> 00:01:06,360
Let's see what it means for a chatbot to predict the next word.

17
00:01:06,820 --> 00:01:10,720
Here I'm using hugging chat to use a model from Mistral AI.

18
00:01:11,080 --> 00:01:17,640
My input prompt will be "twinkle twinkle little" and the completion is * how I wonder

19
00:01:17,960 --> 00:01:19,700
what you are and so on.

20
00:01:20,140 --> 00:01:25,319
This is why the output is called a completion because the early versions of chatbots learned

21
00:01:25,340 --> 00:01:31,320
to take an incomplete start of a sentence and to predict the next words that would complete the

22
00:01:31,440 --> 00:01:36,880
rest of that sentence. So a chatbot takes your input, the prompt, and it doesn't output its entire

23
00:01:37,180 --> 00:01:42,740
completion all at once. It actually outputs a word or part of a word at a time. So given the input

24
00:01:43,120 --> 00:01:55,300
twinkle twinkle little, it outputs star, then it appends its own output to the initial prompt. So

25
00:01:55,440 --> 00:02:00,460
it outputs another word or part of a word, in this case, the word how.

26
00:02:01,080 --> 00:02:05,300
And it repeats this process, appending its own output to the accumulated text,

27
00:02:05,880 --> 00:02:08,759
which is now twinkle twinkle little star how.

28
00:02:09,320 --> 00:02:15,560
And it uses that entire input to help it predict the next word, which is I, and so on and so on.

29
00:02:15,980 --> 00:02:20,420
So you might notice that you're probably not used to writing prompts this way,

30
00:02:20,640 --> 00:02:24,200
because you're used to already writing them as instructions, as in,

31
00:02:24,740 --> 00:02:27,920
please give me information or summarize this piece of text.

32
00:02:28,540 --> 00:02:32,240
But before chatbots were explicitly trained to follow human-like instructions,

33
00:02:32,840 --> 00:02:33,840
like perform this task,

34
00:02:34,380 --> 00:02:37,640
the way they would normally work is they would just try to complete a piece of text.

35
00:02:38,080 --> 00:02:40,080
For example, given twinkle twinkle little,

36
00:02:40,440 --> 00:02:43,720
it tries to figure out that the next word is probably star,

37
00:02:44,780 --> 00:02:49,220
and then the other words following will be how I wonder what you are.

38
00:02:49,600 --> 00:02:53,060
This has implications for how a chatbot performs a task.

39
00:02:53,580 --> 00:02:55,960
Let's revisit that logic problem in more detail.

40
00:02:56,560 --> 00:02:57,720
Here's the logic problem.

41
00:02:58,240 --> 00:03:01,840
My friend and I have eight additional friends who wish to go to a restaurant.

42
00:03:02,180 --> 00:03:03,720
My car fits four people.

43
00:03:04,100 --> 00:03:06,420
My friend's car fits five people.

44
00:03:06,840 --> 00:03:07,660
No one else has cars.

45
00:03:08,160 --> 00:03:10,460
It takes 20 minutes to drive to the restaurant.

46
00:03:10,940 --> 00:03:13,820
Can we all get to the restaurant with our cars in 20 minutes?

47
00:03:14,660 --> 00:03:20,260
So the number of people to transport is me plus my friend plus eight more or 10 people.

48
00:03:20,620 --> 00:03:24,440
The number of seats in the cars is 4 plus 5 or 9.

49
00:03:24,860 --> 00:03:26,280
The correct answer should be no.

50
00:03:28,000 --> 00:03:31,960
Now let's look at the two outputs of the same model when given the same logic problem,

51
00:03:32,320 --> 00:03:34,240
but with the last instruction worded differently.

52
00:03:34,540 --> 00:03:40,120
On the left, where it was prompted with give your answer, then give your reasoning, it says

53
00:03:40,440 --> 00:03:43,340
yes, we can all get to the restaurant within 20 minutes.

54
00:03:43,920 --> 00:03:46,740
It goes through some reasoning that so far appears to be correct.

55
00:03:47,300 --> 00:03:48,000
Then it says this.

56
00:03:48,520 --> 00:03:53,860
This leaves one person remaining who can accompany either you or your friend in their respective cars.

57
00:03:54,460 --> 00:03:58,820
So here is where the reasoning is going wrong in order to justify its answer of yes.

58
00:03:59,420 --> 00:04:03,160
Now let's look at the response on the right side, where the chatbot was prompted with

59
00:04:03,760 --> 00:04:05,540
give your reasoning, then give your answer.

60
00:04:06,120 --> 00:04:09,000
It starts by calculating that there are a total of 10 people,

61
00:04:09,460 --> 00:04:13,000
and also continues with leaving one person without a ride.

62
00:04:13,380 --> 00:04:18,959
It also correctly states that it is not possible to transport all 10 people to the restaurant

63
00:04:19,280 --> 00:04:19,739
in 20 minutes.

64
00:04:20,359 --> 00:04:21,840
Then it gives the answer no.

65
00:04:24,380 --> 00:04:30,060
Now that you know that the chatbot predicts one word at a time, and it can look back at

66
00:04:30,180 --> 00:04:34,960
all of its previously predicted words to predict the next one, you might be able to see what's

67
00:04:35,120 --> 00:04:35,500
going on.

68
00:04:35,960 --> 00:04:40,899
If it gives the answer first, then thinks through the logic problem, it won't be able to use

69
00:04:40,920 --> 00:04:46,360
all of its reasoning to help it give that answer, because it already gave the answer. In fact, in this

70
00:04:46,520 --> 00:04:52,060
case, it's now just trying to justify its original answer, even if the reasoning is wrong. On the

71
00:04:52,200 --> 00:04:56,900
other hand, if you ask it to go through its reasoning first, it can use all of its reasoning

72
00:04:57,400 --> 00:05:02,320
as additional information in order for it to better answer the question. So how can you make

73
00:05:02,540 --> 00:05:08,219
use of this behavior when prompting a chatbot? Well, if some tasks that you can ask a chatbot to do

74
00:05:08,300 --> 00:05:14,460
can help it perform another task, then ask it to perform those helpful tasks first. For example,

75
00:05:14,820 --> 00:05:20,820
you can prompt, give your reasoning first, then give your answer. Another example is, review what

76
00:05:20,960 --> 00:05:27,240
you know about this topic, write an outline, then write an article. Can you think of other examples?

77
00:05:27,760 --> 00:05:32,440
And one more thing. In the field of natural language processing, the task of predicting the

78
00:05:32,560 --> 00:05:38,200
next word is called language modeling. What does it mean to model a language? If you can iteratively

79
00:05:38,280 --> 00:05:43,560
predict the next word in a piece of text and make it sound like human-produced text, then you're

80
00:05:43,800 --> 00:05:48,660
modeling that language. And if you get a model to learn how to predict the next word, then it's a

81
00:05:48,940 --> 00:05:55,600
language model. Very recently, AI researchers found that if they used really large models with a lot

82
00:05:55,680 --> 00:06:00,360
of text data and get them to learn about language by predicting the next word, then these models

83
00:06:00,540 --> 00:06:07,460
could learn to not just predict the next word, but also do many other tasks, from translating to other

84
00:06:07,480 --> 00:06:14,260
languages to summarizing text and more. So that's why these things are called large language models

85
00:06:14,840 --> 00:06:20,680
or LLMs for short. And you can give the large language model a hint about what tasks you wanted

86
00:06:20,840 --> 00:06:26,160
to do within the input itself. If you recall when you used a machine translation tool, you would

87
00:06:26,240 --> 00:06:30,780
just give it a piece of text and it would translate it to another language based on the language you

88
00:06:30,940 --> 00:06:36,220
chose from a drop down menu. With a language model within the text itself, you can give it an

89
00:06:36,240 --> 00:06:41,540
instruction like translate to French followed by a piece of text and the

90
00:06:41,760 --> 00:06:45,860
model will then figure out that you want it to perform a translation task and if

91
00:06:45,920 --> 00:06:50,120
you wanted to do another task like summarize a long movie script you can

92
00:06:50,280 --> 00:06:54,800
include the instruction summarize this movie script as part of your input this

93
00:06:54,920 --> 00:07:00,220
is directing or prompting the model to perform a certain task and that's why

94
00:07:00,600 --> 00:07:03,260
the inputs to large language models are called prompts.

95
00:07:05,220 --> 00:07:10,220
Next up, let's look at what happens when large language models make stuff up that isn't true.

96
00:07:10,600 --> 00:07:12,660
You'll learn about hallucinations next.

